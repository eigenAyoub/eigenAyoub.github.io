<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Kaizen | 10k</title>
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Kaizen" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="10k" />
<meta property="og:description" content="10k" />
<link rel="canonical" href="http://localhost:4000/blogs/ppl/" />
<meta property="og:url" content="http://localhost:4000/blogs/ppl/" />
<meta property="og:site_name" content="Kaizen" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Kaizen" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"10k","headline":"Kaizen","url":"http://localhost:4000/blogs/ppl/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Kaizen" /></head>

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Kaizen</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blogs/embed-fusion/">Models:</a><a class="page-link" href="/gal/">Cuties</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <p>This mini blog answers:</p>

<ol>
  <li>What is the perplexity score.</li>
  <li>How is it implemented, e.g., with Hugging Face models.</li>
  <li>Can we do better than perplexity?</li>
</ol>

<h2 id="quick-wiki">Quick wiki:</h2>

<p>Given a  tokenized sequence \(X = (x_0, x_1, \ldots, x_t)\) and an autoregressive model \(p_{\theta}(. \mid .)\) the perplexity (of \(p_{\theta}\) on \(X\)) is defined as follows:</p>

<div id="eq">
\[
\text{ppl}(X) = \exp 
\left\{ 
-\frac{1}{t} \sum_{i=1}^{t} \log p_{\theta}(x_i \mid x_{&lt; i}) 
\right\}
\]
</div>

<ul>
  <li>The quantity \(p_{\theta}(x_i \mid x_{&lt; i})\) represents the normalized score (i.e., probability) that the model generates the token \(x_i\) after seeing the context \(x_{\lt i} = (x_0, x_1, \ldots, x_{i-1})\).</li>
  <li>In practice, LMs usually outputs the logits (un-normalized scores), for each sequence input, we get a list of scores of size <code class="language-plaintext highlighter-rouge">vocab_size</code>. This is usually done in parallel over all input sequence.</li>
</ul>

<h2 id="code">Code:</h2>

<p>The following code snippet (provided by Hugging Face, see references section) computes the perplexity score of GPT2-small on Lambada. Hidden is the pre-processing code (model loading, tokenizer, and configs) followed by the main loop.</p>

<details>
	<summary>Pre-processing: </summary>
<div>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2LMHeadModel</span><span class="p">,</span> <span class="n">GPT2TokenizerFast</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>  

<span class="n">device</span> <span class="o">=</span> <span class="s">"cuda"</span>

<span class="n">model</span>  <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="sa">f</span><span class="s">"./hf/73150"</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2TokenizerFast</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s">"gpt2"</span><span class="p">)</span>

<span class="n">test</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s">"lambada"</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s">"test"</span><span class="p">)</span>
<span class="n">encodings</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s">"</span><span class="se">\n\n</span><span class="s">"</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">test</span><span class="p">[</span><span class="s">"text"</span><span class="p">]),</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s">"pt"</span><span class="p">)</span>

<span class="n">max_length</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">n_positions</span>
<span class="n">stride</span> <span class="o">=</span> <span class="mi">1024</span>
<span class="n">seq_len</span> <span class="o">=</span> <span class="n">encodings</span><span class="p">.</span><span class="n">input_ids</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="n">nlls</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">prev_end_loc</span> <span class="o">=</span> <span class="mi">0</span>
</code></pre></div>    </div>
  </div>
</details>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">begin_loc</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">stride</span><span class="p">)):</span>
    <span class="n">end_loc</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">begin_loc</span> <span class="o">+</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>
    <span class="n">trg_len</span> <span class="o">=</span> <span class="n">end_loc</span> <span class="o">-</span> <span class="n">prev_end_loc</span>  
	<span class="c1"># may be different from stride on last loop
</span>    <span class="n">input_ids</span> <span class="o">=</span> <span class="n">encodings</span><span class="p">.</span><span class="n">input_ids</span><span class="p">[:,</span> <span class="n">begin_loc</span><span class="p">:</span><span class="n">end_loc</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">target_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">.</span><span class="n">clone</span><span class="p">()</span>
    <span class="n">target_ids</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="n">trg_len</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">100</span>

    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">target_ids</span><span class="p">)</span>
        <span class="n">neg_log_likelihood</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">loss</span>

    <span class="n">nlls</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">neg_log_likelihood</span><span class="p">)</span>

    <span class="n">prev_end_loc</span> <span class="o">=</span> <span class="n">end_loc</span>
    <span class="k">if</span> <span class="n">end_loc</span> <span class="o">==</span> <span class="n">seq_len</span><span class="p">:</span>
        <span class="k">break</span>

<span class="n">ppl</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">nlls</span><span class="p">).</span><span class="n">mean</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="n">ppl</span><span class="p">.</span><span class="n">item</span><span class="p">())</span>
</code></pre></div></div>

<ul>
  <li><strong>Let’s dissect the main loop:</strong></li>
</ul>

<p>By matching the <a href="#eq">equation above</a> and <code class="language-plaintext highlighter-rouge">ppl = torch.exp(torch.stack(nlls).mean())</code>. We understand that <code class="language-plaintext highlighter-rouge">nlls[i]</code> must represent the quantity:</p>

<p>\[
\log p_{\theta}(x_i \mid x_{\lt i})
\]</p>

<ul>
  <li><strong>What is the nature of:</strong> <code class="language-plaintext highlighter-rouge">outputs = model(input_ids, labels=target_ids)</code>.</li>
</ul>

<p>The variable <code class="language-plaintext highlighter-rouge">outputs</code> is of type <code class="language-plaintext highlighter-rouge">transformers.modeling_outputs.CausalLMOutputWithCrossAttentions</code>, and has three keys:</p>

<ol>
  <li><code class="language-plaintext highlighter-rouge">outputs.loss</code>: a single scaler that apparently represnts the negative log likelihood loss of the current sequence.</li>
  <li><code class="language-plaintext highlighter-rouge">outputs.logits</code>: the output matrix of the LM, has a shape of <code class="language-plaintext highlighter-rouge">[1, seq_len, vocab_len]</code>.</li>
  <li><code class="language-plaintext highlighter-rouge">past_key_values</code>: will ignore for now.</li>
</ol>

<ul>
  <li><strong>How exactly can we compute the</strong> <code class="language-plaintext highlighter-rouge">outputs.loss</code> <strong>from the</strong> <code class="language-plaintext highlighter-rouge">outputs.logits</code>:</li>
</ul>

<p>The <code class="language-plaintext highlighter-rouge">output</code> matrix, compute the un-normalized scores of the next token of each input_token over the <code class="language-plaintext highlighter-rouge">vocab_size</code>. Hence, for each element in the sequence, you get a list of size <code class="language-plaintext highlighter-rouge">vocab-size</code> of un-normalized scores over… Also, the <code class="language-plaintext highlighter-rouge">target</code> is exactly a clone of the <code class="language-plaintext highlighter-rouge">input</code>. Hence, the first element of <code class="language-plaintext highlighter-rouge">target</code> shouldn’t be used. But also the last element of <code class="language-plaintext highlighter-rouge">input</code> as we don’t have it’s ground truth at that moment, it would be computed at the next iteration of the loop. Hence, manually, this code snippet should do the job:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># model outputs
</span><span class="n">logits</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">target_ids</span><span class="p">)</span>

<span class="c1"># softmax over dim 0 (over the vocab_sized for each input token)
</span><span class="n">logits_softmax</span>  <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="c1"># for each input token, we gather the score assigned for it's true next token (`logits[input[ind], target[ind+1]` )
</span>


<span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">gather</span><span class="p">(</span><span class="n">logits_softmax</span><span class="p">,</span> <span class="n">indices</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

</code></pre></div></div>

<p>Test:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># code
</span>

<span class="c1"># for this:
</span><span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Manual nll from logits &gt;&gt;  </span><span class="si">{}</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"HF nnl output (output.loss) &gt;&gt; </span><span class="si">{</span><span class="n">outputs</span><span class="p">.</span><span class="n">loss</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>

<p>Hence, for each run:</p>
<ul>
  <li>We are only interested in the model prediction for the <strong>BEFORE</strong> last token, which is <strong>32002</strong> in this example.</li>
  <li>We need to look at <code class="language-plaintext highlighter-rouge">outputs[0,-2]</code> and not <code class="language-plaintext highlighter-rouge">outputs[0,-1]</code>.</li>
  <li><code class="language-plaintext highlighter-rouge">outputs[0, -2]</code> has a <code class="language-plaintext highlighter-rouge">[1, vocab_size]</code> shape. And, <code class="language-plaintext highlighter-rouge">outputs[0, -2][379]</code> would be represent exactly how much weight does the model think that the next token after 32002 would be <strong>379</strong>.</li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">outputs[0, -2]</code> is not normalized. Hence, it should be softmax’d first.</p>
  </li>
  <li><strong>Important note</strong></li>
</ul>

<p>One should be careful as some models implements <code class="language-plaintext highlighter-rouge">input</code> and <code class="language-plaintext highlighter-rouge">target</code> differently. For instance, in Karpathy’s GPT2 implementation, the <code class="language-plaintext highlighter-rouge">target</code> is usually <code class="language-plaintext highlighter-rouge">input[1:]</code> plus the true next token of <code class="language-plaintext highlighter-rouge">input[-1]</code>, where as Hugging Face models, expect <code class="language-plaintext highlighter-rouge">input</code> and <code class="language-plaintext highlighter-rouge">target</code> to be the exact same.</p>

<h2 id="beyond-perplexity">Beyond perplexity:</h2>

<p>Here I thought we discussed</p>

<p>#e References:</p>

<ul>
  <li><a href="https://huggingface.co/docs/transformers/en/perplexity">Hugging Face blog</a></li>
  <li><a href="https://thegradient.pub/understanding-evaluation-metrics-for-language-models/">The gradient blog</a></li>
</ul>


      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>
  <div class="wrapper">
    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">
              Kaizen
          </li>
        </ul>
      </div>
      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/eigenAyoub"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">eigenAyoub</span></a></li><li><a href="https://instagram.com/curl.ayoub"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#instagram"></use></svg> <span class="username">curl.ayoub</span></a></li><li><a href="https://www.linkedin.com/in/benayad"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg> <span class="username">benayad</span></a></li><li><a href="https://www.twitter.com/benayad_"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">benayad_</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>10k</p>
      </div>

    </div>
  </div>
</footer>
</body>

</html>
