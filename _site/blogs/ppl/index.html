<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Kaizen | 10k</title>
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Kaizen" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="10k" />
<meta property="og:description" content="10k" />
<link rel="canonical" href="http://localhost:4000/blogs/ppl/" />
<meta property="og:url" content="http://localhost:4000/blogs/ppl/" />
<meta property="og:site_name" content="Kaizen" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Kaizen" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"10k","headline":"Kaizen","url":"http://localhost:4000/blogs/ppl/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Kaizen" /></head>

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Kaizen</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/gal/">Cuties</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <p>This mini blog answers:</p>
<ol>
  <li>What is the famous  the famous perplexity score?</li>
  <li>How is it implemented, especially in Hugging Face?</li>
</ol>

<h2 id="small-wikimath">Small wiki/math:</h2>

<p>Perplexity is defined as the exponentiated average negative log-likelihood of a sequence. If we have a tokenized sequence \(X = (x_0, x_1, \ldots, x_t)\), then the perplexity of \(X\) is,</p>

\[\text{ppl}(X) = \exp 
\left\{ 
-\frac{1}{t} \sum_{i=1}^{t} \log p_{\theta}(x_i \mid x_{&lt; i}) 
\right\}\]

<h2 id="code">Code:</h2>

<p>The following code snippet computes the perplexity score of GPT2-small on Lambada:</p>

<details>
	<summary> Click show pre-processing (loading the model, dataset, and configs)</summary>
<div>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2LMHeadModel</span><span class="p">,</span> <span class="n">GPT2TokenizerFast</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>  

<span class="n">device</span> <span class="o">=</span> <span class="s">"cuda"</span>

<span class="n">model</span>  <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="sa">f</span><span class="s">"./hf/73150"</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2TokenizerFast</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s">"gpt2"</span><span class="p">)</span>

<span class="n">test</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s">"lambada"</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s">"test"</span><span class="p">)</span>
<span class="n">encodings</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s">"</span><span class="se">\n\n</span><span class="s">"</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">test</span><span class="p">[</span><span class="s">"text"</span><span class="p">]),</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s">"pt"</span><span class="p">)</span>

<span class="n">max_length</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">n_positions</span>
<span class="n">stride</span> <span class="o">=</span> <span class="mi">1024</span>
<span class="n">seq_len</span> <span class="o">=</span> <span class="n">encodings</span><span class="p">.</span><span class="n">input_ids</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="n">nlls</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">prev_end_loc</span> <span class="o">=</span> <span class="mi">0</span>
</code></pre></div>    </div>
  </div>

</details>

<p>The main loop:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">begin_loc</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">stride</span><span class="p">)):</span>
    <span class="n">end_loc</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">begin_loc</span> <span class="o">+</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>
    <span class="n">trg_len</span> <span class="o">=</span> <span class="n">end_loc</span> <span class="o">-</span> <span class="n">prev_end_loc</span>  
	<span class="c1"># may be different from stride on last loop
</span>    <span class="n">input_ids</span> <span class="o">=</span> <span class="n">encodings</span><span class="p">.</span><span class="n">input_ids</span><span class="p">[:,</span> <span class="n">begin_loc</span><span class="p">:</span><span class="n">end_loc</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">target_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">.</span><span class="n">clone</span><span class="p">()</span>
    <span class="n">target_ids</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="n">trg_len</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">100</span>

    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">target_ids</span><span class="p">)</span>
        <span class="n">neg_log_likelihood</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">loss</span>

    <span class="n">nlls</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">neg_log_likelihood</span><span class="p">)</span>

    <span class="n">prev_end_loc</span> <span class="o">=</span> <span class="n">end_loc</span>
    <span class="k">if</span> <span class="n">end_loc</span> <span class="o">==</span> <span class="n">seq_len</span><span class="p">:</span>
        <span class="k">break</span>

<span class="n">ppl</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">nlls</span><span class="p">).</span><span class="n">mean</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="n">ppl</span><span class="p">.</span><span class="n">item</span><span class="p">())</span>
</code></pre></div></div>

<p>Let’s dissect the main loop. Matching the equation above and <code class="language-plaintext highlighter-rouge">ppl = torch.exp(torch.stack(nlls).mean())</code>, <code class="language-plaintext highlighter-rouge">nlls[i]</code> must represent the \(\log p_{\theta}(x_i \mid x_{&lt; i})\).</p>

<p>Question is, what is the <code class="language-plaintext highlighter-rouge">outputs = model(input_ids, labels=target_ids)</code>.</p>

<p>The variable <code class="language-plaintext highlighter-rouge">outputs</code> is of type <code class="language-plaintext highlighter-rouge">transformers.modeling_outputs.CausalLMOutputWithCrossAttentions</code>, and has three keys:</p>

<ol>
  <li><code class="language-plaintext highlighter-rouge">outputs.loss</code>: a single scaler, it represents exactly the quantity \(\log p_{\theta}(x_i \mid x_{&lt; i})\).</li>
  <li><code class="language-plaintext highlighter-rouge">outputs.logits</code>: this is the actual output matrix of the LM, is has a shape of <code class="language-plaintext highlighter-rouge">[1, seq_len, vocab_len]</code>.</li>
  <li><code class="language-plaintext highlighter-rouge">past_key_values</code> will ignore for now.</li>
</ol>

<p>How to compute the <code class="language-plaintext highlighter-rouge">loss</code> from the <code class="language-plaintext highlighter-rouge">logits</code>:</p>

<p>We have:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">target_ids</span><span class="p">)</span>
<span class="c1"># input_ids &gt;&gt; tensor([[  257,  1598,  7815,  ...,  1175, 32002,   379]], device='cuda:0')
# target_ids &gt;&gt; tensor([[-100, -100, -100,  ..., -100, -100,  379]], device='cuda:0')
</span></code></pre></div></div>

<p>Hence, for each run:</p>

<ul>
  <li>We are only interested in the model prediction for the <strong>BEFORE</strong> last token, which is <strong>32002</strong> in this example.</li>
  <li>We need to look at <code class="language-plaintext highlighter-rouge">outputs[0,-2]</code> and not <code class="language-plaintext highlighter-rouge">outputs[0,-1]</code>.</li>
  <li><code class="language-plaintext highlighter-rouge">outputs[0, -2]</code> has a <code class="language-plaintext highlighter-rouge">[1, vocab_size]</code> shape. And, <code class="language-plaintext highlighter-rouge">outputs[0, -2][379]</code> would be represent exactly how much weight does the model think that the next token after 32002 would be <strong>379</strong>.</li>
  <li><code class="language-plaintext highlighter-rouge">outputs[0, -2]</code> is not normalized. Hence, it should be softmaxe’d first.</li>
</ul>

<h2 id="references">References:</h2>

<ul>
  <li><a href="https://huggingface.co/docs/transformers/en/perplexity">Hugging Face blog</a></li>
  <li><a href="https://thegradient.pub/understanding-evaluation-metrics-for-language-models/">The gradient blog</a></li>
</ul>


      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>
  <div class="wrapper">
    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">
              Kaizen
          </li>
        </ul>
      </div>
      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/eigenAyoub"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">eigenAyoub</span></a></li><li><a href="https://instagram.com/curl.ayoub"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#instagram"></use></svg> <span class="username">curl.ayoub</span></a></li><li><a href="https://www.linkedin.com/in/benayad"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg> <span class="username">benayad</span></a></li><li><a href="https://www.twitter.com/benayad_"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">benayad_</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>10k</p>
      </div>

    </div>
  </div>
</footer>
</body>

</html>
