<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Kaizen | 10k</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Kaizen" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="10k" />
<meta property="og:description" content="10k" />
<link rel="canonical" href="http://localhost:4000/blogs/TD-learning/" />
<meta property="og:url" content="http://localhost:4000/blogs/TD-learning/" />
<meta property="og:site_name" content="Kaizen" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Kaizen" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"10k","headline":"Kaizen","url":"http://localhost:4000/blogs/TD-learning/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Kaizen" /></head>

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Kaizen</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/gal/">Cuties</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <p>Keywords: RL, Q-learning, stochastic approximation.</p>

\[p(s',r \lvert s,a)=\Pr \{S_t=s',R_t=r \lvert S_{t-1}=s,A_{t-1}=a\}\]

<h2 id="proof-of-convergence-of-q-learning">Proof of convergence of Q-Learning</h2>

<p>In this blogpost, I’ll write about what I have learned regarding the asymptomatic guarentees of Q-Learning and TD-Learning. Eventhough we are going to use some results from Stochastic Approximation (SA) theory without a proof, going through this material made me appriciate Q-learning way more than just a toy tool.</p>

<p>It is also fair to meantion that the “asymtotic” convergence of Q-Learning is fairly *EASY* compared to finding guanrentees on the * RATE *\ of convergence, which is still an active field of research, I guess! Props to you RL theory folks!</p>

<p>At first I’ll formulate the problem setting in a <em>SA framework</em>, state the conditions and assumptions, and  finally I’ll the main theorem to the case of Q-learning.</p>

<h2 id="content">Content:</h2>
<ul>
  <li><a href="#setting">Setting</a></li>
  <li><a href="#algorithm">Algorithm</a></li>
  <li><a href="#proof">Proof</a>
    <ul>
      <li><a href="#sketch">Sketch</a></li>
      <li><a href="#proof">Proof</a></li>
    </ul>
  </li>
  <li><a href="#references">References</a></li>
</ul>

<h2 id="stochastic-approximation-of-a-contractive-operator">Stochastic Approximation of a contractive operator:</h2>

<p>Let \(L: R^d \longrightarrow R^d\) be a \(c\)-lipschitz function where \(c&lt;1\) (This obviously implies contraction, which implies the fixed point theorem in Banach Spaces). We are interested in finding  \(\theta^*\) the fixed point of \(L\), i.e., \(L \theta^* = \theta^*\).</p>

<blockquote>
  <p>Bear in mind that the Q-learning actor that would assume the role of \(L\) is the Bellman Optimality Operator \(T^*\). As you know the \(T^*\) is a contraction mapping for the infinite? — check this out today</p>
</blockquote>

<blockquote>
  <p>While \(\theta_{t}\) would be replaced by \(Q_t(.,.)\), therefore, the dimension \(d\) of \(\theta_{t}\) should match the state-action space cardinal \(\lvert X x A \lvert\).</p>
</blockquote>

<p>With the right conditions on \(\{ \alpha_{t} \}_{t=&gt;0}\) (Known as the Monro … SA conditions 1 below, kn), having access to \(L\) guarantees us to land in \(\theta^*\) using the following (smooth) interative update:</p>

\[\theta_{t+1} = (1-\alpha_t) \theta_{t} + \alpha_t L \theta_{t}\]

<p>SA conditions:</p>
<ol>
  <li>Divergence of  \(\{ \alpha_{t} \}_{t=&gt;0}\)</li>
  <li>Convergence of  \(\{ \alpha_{t}^2 \}_{t=&gt;0}\)</li>
</ol>

<p>Intuitively speaking, we the want … to slowly diverge.</p>

<p>The first condition ensures the mean, the second is crutial to make the variance of the estimate vanish to 0.</p>

<p>The past result is a straight application of the fixed point theorem in Banach Spaces, for \(L' = (1- \alpha)I + \alpha L\).</p>

<p>Having access to \(L\) is a luxary we can’t generally afford in online settings, where the model dynamics are agnostic. A more realistic situation is to only have access to a noisy *unbiased*\ estimate, namely \(L \theta_{t} +  \eta_{t}\) with \(\mathbb{E}[\eta_{t}] = 0\).</p>

<p>To make it even more interesting, we want to update each componenent apart, \(\theta_{t}^i\) for \(i \partof [1:d]\) as follows:</p>

<table>
  <tbody>
    <tr>
      <td>\(\alpha_{j}\) = 0 for $$ j</td>
      <td>= i $$   [#]</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>With our analogy in mind, in Q-Learning, we don’t update Q_t synchronously for all state-action pair.</p>
</blockquote>

<p>Therefore, we want to prove that the sequence of \(\theta_{t}\) generated by  procedure above would eventully converge to the fixed point? the answer is YES [x-ref], provided some assumptions of course. Nothing comes for free in this business! Namely o the noise \(\etha\)</p>

<p>Let’s call them Assuption A1 following [1] notations:</p>

<p>Assumption A1:
1- Regarding the mean:
This assuption makes the estimate unbiased</p>

<p>2 Regarding the variance:</p>

<p>Now let’s state the theorem: Convergence of stochastce approximation:</p>

<p>Usually, the conditions in A2, especially the iones regarding the variance are the “hardest” to prove. The other ones are usually obvious. The operator would usually be T_star which a contraction w.r.t the inf_norm and the {alpha_t} sastifies the SA conditions by design.</p>

<p>Let’s see in Q-Learning for instance. The update rule looks like:</p>

<hr />

<p>When re-formulated:</p>

<hr />

<p>N_t = ….</p>

<p>we get …</p>

<p><strong>Remark How mayn times do we need to visit a pair \(\( X,A \)\)</strong></p>

<p>A condition that we never stated regarding how we explore the state-action space. As the \alpha_{t} needs to be verified for each parin \((X,A)\), this requires us to visit each state infinelty many… otherwise the first condition wouldn’t be verified.</p>

<p>Of course this is ony an asymptotic guaarntee.</p>

<h2 id="setting">Setting:</h2>
<p>We have an MDPs \(\)
Take it from Preux Papers, Check 2 or 3 papers</p>

<h2 id="algorithm">Algorithm:</h2>

<p>The interaction between the agent and its environment is captured through an MDP, a 5-tuple &lt;X,A, P, R, gamma&gt;. WE aim to learn an Optimal Policy by first learning the Optimal Action-Value function, Q^* for each pair x,a in X x A.</p>

<p>The asynchronous Q-Learning is as follows:</p>

<p>Note that this is an off-policy RL algorithm, as the behavioral p[olicy differes fro the one we are trying evaluaatin (\pi_{g}(Q_t)</p>

<h2 id="proof">Proof:</h2>
<p>\(\begin{aligned}
V(s) &amp;= \mathbb{E}[G_t \vert S_t = s] \\
&amp;= \mathbb{E} [R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots \vert S_t = s] \\
&amp;= \mathbb{E} [R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \dots) \vert S_t = s] \\
&amp;= \mathbb{E} [R_{t+1} + \gamma G_{t+1} \vert S_t = s] \\
&amp;= \mathbb{E} [R_{t+1} + \gamma V(S_{t+1}) \vert S_t = s]
\end{aligned}\)</p>

<h3 id="sketch">Sketch:</h3>

<h3 id="proof-1">Proof:</h3>
<p>Let this and that be …</p>

<h2 id="references">References:</h2>
<ul>
  <li>[] Neuro dynamic programming</li>
  <li>[] Link to TD-learnig paper</li>
  <li>[] Amir massoud Fahramad</li>
  <li>[] Link to youtube playlist, github linl</li>
</ul>


      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>
  <div class="wrapper">
    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">
              Kaizen
          </li>
        </ul>
      </div>
      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/eigenAyoub"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">eigenAyoub</span></a></li><li><a href="https://instagram.com/curl.ayoub"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#instagram"></use></svg> <span class="username">curl.ayoub</span></a></li><li><a href="https://www.linkedin.com/in/eigenAyoub"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg> <span class="username">eigenAyoub</span></a></li><li><a href="https://www.twitter.com/eigenAyoub"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">eigenAyoub</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>10k</p>
      </div>

    </div>
  </div>
</footer>
</body>

</html>
