<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Kaizen | 10k</title>
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Kaizen" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="10k" />
<meta property="og:description" content="10k" />
<link rel="canonical" href="http://localhost:4000/blogs/fastinference/" />
<meta property="og:url" content="http://localhost:4000/blogs/fastinference/" />
<meta property="og:site_name" content="Kaizen" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Kaizen" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"10k","headline":"Kaizen","url":"http://localhost:4000/blogs/fastinference/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Kaizen" /></head>

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Kaizen</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/gal/">Cuties</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <p>– Last update: 08 Dec 2023.</p>

<p>I’ll collect in the blog a few recent trends that help speed up inference of autoregressive models.</p>

<hr />
<h2 id="jump-to-it">Jump to it:</h2>

<ul>
  <li><a href="#built-different">Wait</a>, I thought Transformers are <a href="https://www.urbandictionary.com/define.php?term=Built%20Different">Built Different</a>!</li>
  <li><a href="#mqa">Multi query attention.</a></li>
  <li><a href="#gqa">Grouped query Attention.</a></li>
  <li><a href="#sliding-window">Sliding window Attention.</a></li>
  <li>Flash Attention.</li>
  <li><a href="#mistral-7b">Notes on Mistral 7B.</a></li>
  <li><a href="#speculative-decoding">Speculative decoding.</a></li>
</ul>

<hr />
<h3 id="-what-happened-to-transformers-are-very-optimalparallelizable-"><strong>&gt; What happened to “Transformers are very optimal/parallelizable”</strong> <a name="built-different"></a></h3>

<p>Ok, chill!</p>

<p>That is still the case, and will always be. But we mostly benefit from it in <strong>TRAINING</strong>. Inference though, is <strong>inherently</strong> sequential. I.e., say that \(p(. \mid .)\) represents the output distribution of the model, and given a prompt \(T\), you generate the first token autoregressively by sampling \(x_1 \sim p(.\mid T)\) and then to generate the second token you condition on both \([T, x_1]\), \(x_2 \sim p(.\mid T, x_1)\). There is simply (and sadly) no escape from this incremental design.</p>

<p>I guess no one is perfect after all, right?!</p>

<hr />
<h3 id="-multi-query-attention-mqa-"><strong>&gt; Multi Query Attention (MQA):</strong> <a name="mqa"></a></h3>

<p>This is a <a href="https://arxiv.org/pdf/1911.02150.pdf">great paper</a>, its idea could be summarized in <a href="#tldr">one sentence</a>, but the performance analyses that he provides are super interesting. I’ll only detail the analysis of the first section.</p>

<p>As a motivation, <a href="https://blog.google/technology/ai/google-gemini-ai/">Gemini 1.0</a> was released on Dec 06 2023. Although they don’t disclose much about the architectural choices behind it, they made it clear that they’ve used MQA (pretty cool, huh?). As you see below, screen taken from the <a href="https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf">technical report</a>.</p>

<blockquote>
  <p><img src="/src/mistral/MQA-Gemini.png" alt="MQA-gemini" /></p>
</blockquote>

<p><strong>Content:</strong></p>
<ul>
  <li><a href="#batched-mha">Batched Multi-Head Attention (MHA).</a></li>
  <li><a href="#batched-mha-inference">Incremental Batched MHA.</a></li>
  <li><a href="#batched-mqa">Multi-Query Attention (MQA).</a></li>
  <li><a href="#incremental-batched-mqa">Incremental MQA.</a></li>
</ul>

<p><a name="tldr"></a>
<strong>TL;DR:</strong> Use only <strong>ONE</strong> key/value per multiple queries (i.e., drop the <code class="language-plaintext highlighter-rouge">dim</code> across the heads of <code class="language-plaintext highlighter-rouge">K</code> and <code class="language-plaintext highlighter-rouge">V</code>).</p>

<p><strong>– Batched Multi-Head Attention (MHA, aka, Vanilla Attention):</strong>  <a name="batched-mha"></a></p>
<ul>
  <li>Notation:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">model_size</code> = d</li>
      <li><code class="language-plaintext highlighter-rouge">batch_size</code> = b  (number of sequences)</li>
      <li><code class="language-plaintext highlighter-rouge">seq_len</code> = n</li>
      <li><code class="language-plaintext highlighter-rouge">P_k, P_v, P_q</code>: Projections matrics, with size \(hdk\), \(hdv\), and \(hdk\) respectively.</li>
      <li>\(X, M\): Input and context matrices (sizes: \(bnd, bnd\))
        <ul>
          <li>Careful, \(M\) is used with masking.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Computing <strong>\(Q, K\) and \(V\)</strong>:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">Q</code> =\(X\) <code class="language-plaintext highlighter-rouge">@</code> \(P_K\) (size: \(bhnk\))
        <ul>
          <li>Think of the dimension as follows: In each head \(h\), each token input \(n\) in seq \(b\), has a query \(q\) representation of dimension \(k\))</li>
        </ul>
      </li>
      <li><code class="language-plaintext highlighter-rouge">K</code> =\(M\) <code class="language-plaintext highlighter-rouge">@</code> \(P_K\) (size: \(bhnk\))</li>
      <li><code class="language-plaintext highlighter-rouge">V</code> =\(M\) <code class="language-plaintext highlighter-rouge">@</code> \(P_V\) (size: \(bhnk\), using \(k=v\))</li>
    </ul>
  </li>
  <li>I’ll ignore the element-wise operations, as those do not change in neither MHA or MQA.</li>
</ul>

<p>Here’s the provided script for completeness: <a name="mha-script"></a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">MultiheadAttentionBatched</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">P_q</span><span class="p">,</span> <span class="n">P_k</span><span class="p">,</span> <span class="n">P_v</span><span class="p">,</span> <span class="n">P_o</span><span class="p">):</span>
    <span class="s">"""
    [...]
    P_q: a tensor with shape [h, d, k] # keep a look at the dim
    P_k: a tensor with shape [h, d, k] # keep a look at the dim
    P_v: a tensor with shape [h, d, v] # keep a look at the dim
    P_o: a tensor with shape [h, d, v]
    [...]
    """</span>

    <span class="n">Q</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">einsum</span><span class="p">(</span><span class="s">"bnd,hdk-&gt;bhnk"</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">P_q</span><span class="p">)</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">einsum</span><span class="p">(</span><span class="s">"bmd,hdk-&gt;bhmk"</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">P_k</span><span class="p">)</span>
    <span class="n">V</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">einsum</span><span class="p">(</span><span class="s">"bmd,hdv-&gt;bhmv"</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">P_v</span><span class="p">)</span>
    <span class="p">[...]</span>
    <span class="c1"># the rest in un-changed in MQA.
</span>    
</code></pre></div></div>

<p><strong>– Performance analysis of Batched MHA:</strong></p>

<ul>
  <li>Total number of operations =  \(\Theta(b n d^2)\), why \(\downarrow\)
    <ul>
      <li>Let’s check \(Q\) for instance, we have <code class="language-plaintext highlighter-rouge">Q</code> = \(X\) <code class="language-plaintext highlighter-rouge">@</code> \(P_K\)</li>
      <li>\(X \rightarrow (bn \times d)\) and \(P_K \rightarrow (d \times hk)\)</li>
      <li>\(\implies Q\) takes \(\mathcal{O}(bn \times d \times hk)  = \mathcal{O}(bn d^2)\) ops as \(hk = d\) (cute, innit?)</li>
    </ul>
  </li>
  <li>Total size of memory (to be accessed): \(b n d + b h n^2 + d^2\). Why \(\downarrow\)
    <ul>
      <li>It’s just the size of the dimensions of all the actors:
        <ul>
          <li>First term \(\rightarrow X, M, Q, K, V, O\) and, \(Y\)</li>
          <li>Second term for the point-wise ops \(\rightarrow\) logits, and softmax.</li>
          <li>Third term \(\rightarrow P_k, P_v, P_q\) and \(P_o\)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">Ratio</code> \(= \frac{\text{memory size}}{\text{# ops}} = \mathcal{O}(\frac{1}{k} + \frac{1}{bn})\)
    <ul>
      <li>The opposite of this ratio is known as <strong><a href="https://docs.nvidia.com/deeplearning/performance/dl-performance-gpu-background/index.html#understand-perf">Arithmetic Intensity</a></strong> (Not really but kinda related).</li>
      <li>Just keep in mind that it is necessary to keep this <code class="language-plaintext highlighter-rouge">ratio</code> \(\ll 1\).</li>
    </ul>
  </li>
  <li>Final note:
    <ul>
      <li>This setting (Batched MHA) is what happens in training. We can see that as long as our \(nb\) is high, we are guarenteed to be in that <code class="language-plaintext highlighter-rouge">ratio</code> \(\ll 1\) regime. Pretty cool, right?</li>
    </ul>
  </li>
</ul>

<p><strong>– Incremental batched MHA (Inference):</strong> <a name="batched-mha-inference"></a></p>

<p>Same as before, except here, we introduce and append  the <code class="language-plaintext highlighter-rouge">KV cache</code> along the way.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">MHA</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">prev_K</span><span class="p">,</span> <span class="n">prev_V</span><span class="p">,</span> <span class="n">P_q</span><span class="p">,</span> <span class="n">P_k</span><span class="p">,</span> <span class="n">P_v</span><span class="p">,</span> <span class="n">P_o</span><span class="p">):</span>
    <span class="s">"""Multi-Head Self-Attention (one step).
    
    Args:
    x: a tensor with shape [b, d]
    prev_K: a tensor with shape [b, h, m, k]
    prev_V: a tensor with shape [b, h, m, v]
    P_q: a tensor with shape [h, d, k]
    P_k: a tensor with shape [h, d, k]
    P_v: a tensor with shape [h, d, v]
    P_o: a tensor with shape [h, d, v]
    
    Returns:
    y: a tensor with shape [b, d]
    new_K: a tensor with shape [b, h, m+1, k]
    new_V: a tensor with shape [b, h, m+1, v]
    """</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">einsum</span><span class="p">(</span><span class="s">"bd,hdk-&gt;bhk"</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">P_q</span><span class="p">)</span>
    <span class="n">new_K</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">prev_K</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span>
		<span class="n">tf</span><span class="p">.</span><span class="n">einsum</span><span class="p">(</span><span class="s">"bd,hdk-&gt;bhk"</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">P_k</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">new_V</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">prev_V</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span>
		<span class="n">tf</span><span class="p">.</span><span class="n">einsum</span><span class="p">(</span><span class="s">"bd,hdv-&gt;bhv"</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">P_v</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">einsum</span><span class="p">(</span><span class="s">"bhk,bhmk-&gt;bhm"</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">new_K</span><span class="p">)</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">o</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">einsum</span><span class="p">(</span><span class="s">"bhm,bhmv-&gt;bhv"</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">new_V</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">einsum</span><span class="p">(</span><span class="s">"bhv,hdv-&gt;bd"</span><span class="p">,</span> <span class="n">o</span><span class="p">,</span> <span class="n">P_o</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">y</span><span class="p">,</span> <span class="n">new_K</span><span class="p">,</span> <span class="n">new_V</span>
</code></pre></div></div>

<p><strong>– Analysis of Incremental batched MHA (Inference):</strong></p>

<p>Similarily to the analysis in the <a href="#batched-MHA">batched MHA</a>, we get for \(n\) generated tokens:</p>

<ul>
  <li><strong>#ops</strong> =  \(\Theta (bnd^2)\)</li>
  <li><strong>#memory</strong> = \(\Theta (b n^2 d + n d^2)\)</li>
  <li><strong><code class="language-plaintext highlighter-rouge">ratio</code></strong> = \(\mathcal{O}(\frac{n}{d} + \frac{1}{b})\) <a name="frac-problem"></a></li>
</ul>

<p>Now, it’s tricky to push the <code class="language-plaintext highlighter-rouge">ratio</code> to be \(\ll 1\). We can’t just increase the batch size \(b\) as we are contsrained by the memory size. But also, when \(n \approx d\), memory bandwidth would be a bottleneck for performance. So, what do we do now? The author proposes, <strong>Multi-Query Attention (MQA)</strong>. MQA radically removes  the heads (<code class="language-plaintext highlighter-rouge">h</code>) dimension off \(K\) and \(V\).</p>

<p><strong>– Multi Query Attention (MQA):</strong> <a name="batched-mqa"></a></p>

<p>As you can see in the script below, MQA is literally MHA without the <code class="language-plaintext highlighter-rouge">h</code> dimension in <code class="language-plaintext highlighter-rouge">K</code>, and <code class="language-plaintext highlighter-rouge">V</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">MultiQueryAttBatched</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">P_q</span><span class="p">,</span> <span class="n">P_k</span><span class="p">,</span> <span class="n">P_v</span><span class="p">,</span> <span class="n">P_o</span><span class="p">):</span>
    <span class="s">"""
    Args:
    [...]
    P_k: a tensor with shape [d, k]
    P_v: a tensor with shape [d, v]
    [...]
    """</span>
    <span class="n">Q</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">einsum</span><span class="p">(</span><span class="s">"bnd,hdk-&gt;bhnk"</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">P_q</span><span class="p">)</span> <span class="c1">#h is still there in the output dim
</span>    <span class="n">K</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">einsum</span><span class="p">(</span><span class="s">"bmd,dk-&gt;bmk"</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">P_k</span><span class="p">)</span>   <span class="c1">#h is dropped 
</span>    <span class="n">V</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">einsum</span><span class="p">(</span><span class="s">"bmd,dv-&gt;bmv"</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">P_v</span><span class="p">)</span> <span class="c1">#h is dropped 
</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">einsum</span><span class="p">(</span><span class="s">"bhnk,bmk-&gt;bhnm"</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span>
    <span class="c1"># we recoved the same dim of the logits as the MHA,
</span>    <span class="c1"># everything is the same as MHA from now onwards.
</span>    <span class="p">[...]</span>
</code></pre></div></div>

<p><strong>– Incremental MQA (Inference)</strong> <a name="incremental-batched-mqa"></a>
Differences in code are marked as <code class="language-plaintext highlighter-rouge">#no h</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">MultiquerySelfAttentionIncremental</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">prev_K</span><span class="p">,</span> <span class="n">prev_V</span><span class="p">,</span> <span class="n">P_q</span><span class="p">,</span> <span class="n">P_k</span><span class="p">,</span> <span class="n">P_v</span><span class="p">,</span> <span class="n">P_o</span><span class="p">):</span>
    <span class="s">"""
    Args:
    x: a tensor with shape [b, d]
    prev_K: a tensor with shape [b, m, k]  #no h
    prev_V: a tensor with shape [b, m, v] #no h
    P_q: a tensor with shape [h, d, k]
    P_k: a tensor with shape [d, k] #no h
    P_v: a tensor with shape [d, v] #no h
    P_o: a tensor with shape [h, d, v]
    [...]
    new_K: a tensor with shape [b, m+1, k] #no h
    new_V: a tensor with shape [b, m+1, v] #no h
    """</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">einsum</span><span class="p">(</span><span class="s">"bd,hdk-&gt;bhk"</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">P_q</span><span class="p">)</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">prev_K</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">einsum</span><span class="p">(</span><span class="s">"bd,dk-&gt;bk"</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">P_k</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">V</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">prev_V</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">einsum</span><span class="p">(</span><span class="s">"bd,dv-&gt;bv"</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">P_v</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">einsum</span><span class="p">(</span><span class="s">"bhk,bmk-&gt;bhm"</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span> 
    <span class="c1"># we revoved the dim `bhm`, the rest is same as before MHA.
</span>    <span class="p">[...]</span>
</code></pre></div></div>

<p><strong>– Performance analysis for incremental MQA (Inference)</strong></p>

<p>For \(n\) generations:</p>

<ul>
  <li><strong>#ops</strong> =  \(\Theta (bnd^2)\)</li>
  <li><strong>#memory</strong> = \(\Theta (b n^2 d + b n^2 k +  n d^2)\)</li>
  <li><strong><code class="language-plaintext highlighter-rouge">ratio</code></strong> = \(\mathcal{O}(\frac{1}{d} + \frac{n}{dh} + \frac{1}{b})\)</li>
</ul>

<p>Here we see that the problematic fraction \(\frac{n}{d}\) that we encountered in <a href="#frac-problem">incremenral-MHA</a>, is further devided by <code class="language-plaintext highlighter-rouge">h</code>… which helps tremendessly with performance.</p>

<p>I spent too much on this paper, here are some final notes:</p>

<ul>
  <li><a href="https://arxiv.org/pdf/2204.02311.pdf">PaLM</a> uses MQA. (see screenshot below from original paper, page: 5).
<img src="/src/mistral/palm.png" alt="palm" /></li>
  <li>One could argue that this improves throughput (we drop memory reqs, hence, we can increase <code class="language-plaintext highlighter-rouge">b</code>?), rather than directly tackling latency.</li>
  <li>MQA can lead to training instability in fine-tuning, especially with long input tasks.
We notice some quality degradation and training instabiity with this method.</li>
  <li>Finally, to benefit from inference speed using MQA, models need to be trained on MQA, which is inconvenient. We would love a way to take advantage of MHA in training, but benefit from  MQA in inference, and this is exactly what you get with <a href="https://arxiv.org/pdf/2305.13245.pdf">Grouped Query Attention</a>, presented next.</li>
</ul>

<hr />
<h3 id="-grouped-query-attention-"><strong>&gt; Grouped Query Attention:</strong> <a name="gqa"></a></h3>

<ul>
  <li><a href="https://arxiv.org/pdf/2305.13245.pdf">This Paper</a> provides two contributions:
    <ol>
      <li>A way to uptrain models with MHA into MQA.
        <ul>
          <li>\(\implies\) no need to train models from the start with MQA, you can just use MHA then switch in inference to MQA.</li>
        </ul>
      </li>
      <li>A generalization to MQA.</li>
    </ol>
  </li>
  <li>Note: This switch in training/architecture from MHA \(\rightarrow\) MHA is a Procedure known in the literature as Upcycling/Uptraining (<a href="https://arxiv.org/pdf/2212.05055.pdf">Check this paper for detais</a>):</li>
</ul>

<p><strong>– Uptrainig from MHA to MQA happens in 2 steps:</strong></p>
<ol>
  <li>Convert checkpoint (snapshot of weights)
    <ul>
      <li>Key and Value projection matrices are mean pooled into a single (<a href="#proj-fig">see screen 2 below</a>).</li>
    </ul>
  </li>
  <li>Additional pre-training on a small percentage \(\alpha\) to adapt to the new structure.
    <ul>
      <li>They uptrained with \(\alpha = 0.05\) of the original pre-training steps of 
<a href="https://jmlr.org/papers/volume21/20-074/20-074.pdf">T5</a>.</li>
    </ul>
  </li>
</ol>

<p><img src="/src/mistral/projection.png" alt="projection" /></p>

<p>Final interesting takes:</p>
<ul>
  <li>They do not evaluate on classification benchmarks, as GLUE (reason: Autoreg. inference is weak/less applicable to these tasks).</li>
  <li>Uptraining for 5% is optimal for both MQA and GQA, pushing it higher (10%) did not help much.</li>
</ul>

<p>Coincid.. Gemini just dropped, and one of the few details about the architeture is that they used MQA… , pretty cool, huh??</p>

<p><strong>– Grouped Query Attention:</strong></p>

<p>Instead of all queries of an attention layer having the same key/value (MQA), we group queries into sub-groups that share the key/value (<a href="#comp-fig">Check screen 1 below</a>).</p>

<p><strong>Comparison figure:</strong>   <a name="comp-fig"></a>
<img src="/src/mistral/comparison.png" alt="comparison" /></p>

<p><strong>– Final notes:</strong></p>

<p>GQA  » Increase in inference speed &amp;&amp; Reduces memory requirement during decoding. 
     » Higher batch size » Higher throughput</p>

<hr />
<h3 id="-sliding-window-attention-"><strong>&gt; Sliding Window Attention:</strong> <a name="sliding-window"></a></h3>
<p><strong>TODO</strong></p>

<p>Relevent literature: <a href="https://arxiv.org/pdf/2004.05150.pdf">Longformer</a>, <a href="https://arxiv.org/pdf/1904.10509.pdf">Sparse Transformers</a>.</p>

<p>Vanilla attention scales quadratically with long sequences (Quick Why: with each input of the sequence (length n), you compute n weight/ reformulate this.)</p>

<p><strong>– Longformer</strong> attention combines:</p>
<ol>
  <li>Windowed attention (local) \(\rightarrow\) build a contextualized representation.</li>
  <li>End task global attention  \(\rightarrow\) build full sequence representations for prediction.</li>
</ol>

<p>Attention pattern:</p>
<ol>
  <li>Sliding windon of fixed size around each token \(\implies\) attention matrix is sparsified.
    <ul>
      <li>Q: Is it literally masked? Q = Q’ x Mask ?  Yes/No.</li>
    </ul>
  </li>
  <li>x</li>
</ol>

<p><strong>– Sparse Transformers:</strong></p>

<p><strong>TL;DR:</strong> Add more sparsity to you attention weights. By shortening your attention span. Instead of accessing all you past, access only up to a window \(W\) (not really, they defind a pattern of “attendence”, will get to it later, the change the usual mask).</p>

<p>Section 4.2 is so well written, that I’m technically going  to be re-writing it.</p>

<p>Vanilla attention (this is a mathpix test, looking fine on markdown so far):</p>

\[\begin{gathered}
\operatorname{Attend}(X, S)=\left(a\left(\mathbf{x}_i, S_i\right)\right)_{i \in\{1, \ldots, n\}} \\
a\left(\mathbf{x}_i, S_i\right)=\operatorname{softmax}\left(\frac{\left(W_q \mathbf{x}_i\right) K_{S_i}^T}{\sqrt{d}}\right) V_{S_i} \\
K_{S_i}=\left(W_k \mathbf{x}_j\right)_{j \in S_i} \quad V_{S_i}=\left(W_v \mathbf{x}_j\right)_{j \in S_i}
\end{gathered}\]

<hr />
<h3 id="-flash-attention"><strong>&gt; Flash Attention:</strong></h3>
<p><strong>TODO</strong></p>

<p>This 34 pages idea, wins the price of “Minimum innovation, maximum results– Ilya Sutskever”</p>

<p>One of the ideas:  Basically, instead of moving KV’s around, just recompute them at backprop.</p>

<p>Why: Compute capabilities on GPUs is 2 orders of magnitude higher than memory. recompute is easy, moving data and storing it in GPUs is hard. 
Q: I thought OpenAI was already doing this, in their 2019 paper. Investigate this!!</p>

<hr />
<h3 id="-notes-on-mistral-7b"><strong>&gt; Notes on Mistral 7B:</strong><a name="mistral-7b"></a></h3>

<p>The ratio of \(\frac{\text{ideas}}{\text{pages}}\) in the <a href="https://arxiv.org/pdf/2310.06825.pdf">Mistral 7B</a> is too high. It combines efficiently all the techniques that are mentioned above. Additionally, they use:</p>

<ul>
  <li><strong>Rolling buffer Cache:</strong></li>
</ul>

<p>Set the cache size to a value \(W\), and store the keys and values at step \(i\) at position \(i \equiv W\) (modulo) \(\implies\) starting from step \(W\), cache get overwritten with new values.</p>

<p>Why is it fine to do so: because we add  <strong>positional encoding</strong> before, so it doesn’t really matter where do you store, attention is a “set” operation.</p>

<ul>
  <li><strong>Pre-fill and Chunking:</strong></li>
</ul>

<p>I don’t get this yet. Check it later!</p>

<p>(Of course it’s related to KV cache.)</p>

<p>(pre-fill, is computing the KV of the prompt, which is already available in advance, hence we can chunk it, etc.)</p>

<p>(Chunking: Split the prompt size in chunks of size \(W\) (window size of the attention)  » Then what?)</p>

<hr />
<h3 id="-speculative-decoding--"><strong>&gt; Speculative decoding</strong>:  <a name="speculative-decoding"></a></h3>

<p><strong>– Quick Terminology:</strong></p>
<ul>
  <li>Target model: A big model (the one we want to eventually optimize), e.g., Llama 70B</li>
  <li>Draft model: A small/fast model, e.g., Llama 7B</li>
</ul>

<p><strong>– Main obsevation:</strong></p>
<ul>
  <li>Time wise, scoring a short continuation of the draft model \(\approx\) Generating 1 token by target model.</li>
</ul>

<p><strong>– Main idea:</strong></p>
<ul>
  <li>Generate tokens with draft model and verify along the way with target model, and correct when necessary.</li>
</ul>

<blockquote>
  <p><strong>Autoregressive sampling is memory bandwidth bound:</strong></p>
</blockquote>

<ul>
  <li>Mainly becasue we load huge parameters, and construct a huge KV cache</li>
  <li><code class="language-plaintext highlighter-rouge">Latency</code> \(\displaystyle \propto\) <code class="language-plaintext highlighter-rouge">params_size</code> X <code class="language-plaintext highlighter-rouge">transformer_memory_size</code>
    <ul>
      <li>Looks cute, but why?</li>
    </ul>
  </li>
  <li>With longer context, your KV cache takes over, and optimizing compute doesn’t cut it anymore. I.e., you’re stuck in memory bound regime (<a href="https://horace.io/brrr_intro.html">Check the original “Making DL go brrr” blog for more.</a>).</li>
</ul>

<blockquote>
  <p><strong>Why is scroring a draft continuation has \(\sim\) latency to generating a token.</strong></p>
</blockquote>

<p><strong>TODO</strong></p>
<ul>
  <li>Input: Prompt \([x_1, \dots, x_{T-1}]\)
    <ul>
      <li>Task 1: Generate the next token \(x_{T}\)</li>
      <li>Task 2: Verify is some token \(q_{T}\) is a plausible generation.</li>
    </ul>
  </li>
</ul>

<p>Imagine having a <strong>draft model</strong> (typically fast/small), and a <strong>target model</strong>, typically big (think of the former as a Llama 7B, and the latter as a Llama 70B).</p>

<blockquote>
  <p><strong>Speculative decoding in a nutshell:</strong></p>
</blockquote>

<ol>
  <li>Generate a sequence of \(K\) (~ 4/5) tokens using the draft model.</li>
  <li>Scoring the the sequence using the target model.</li>
  <li>Accept/Reject the proposed sequence using a rejection sampling inspired technique, that hopefully recovers the target model output distribution.</li>
</ol>

<p>Useful links:</p>
<ul>
  <li><a href="https://arxiv.org/pdf/2302.01318.pdf">Deepmind paper.</a></li>
  <li><a href="https://arxiv.org/pdf/2211.17192.pdf">Same idea by another team from Google.</a></li>
</ul>

<hr />
<p>TODO:</p>

<ul>
  <li>verify the links/ in the contents.</li>
  <li>Proof-Check MQA.</li>
  <li>Have a sync’ed way of listing the references for each section.</li>
  <li>Check, and update.</li>
</ul>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>
  <div class="wrapper">
    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">
              Kaizen
          </li>
        </ul>
      </div>
      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/eigenAyoub"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">eigenAyoub</span></a></li><li><a href="https://instagram.com/curl.ayoub"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#instagram"></use></svg> <span class="username">curl.ayoub</span></a></li><li><a href="https://www.linkedin.com/in/benayad"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg> <span class="username">benayad</span></a></li><li><a href="https://www.twitter.com/benayad_"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">benayad_</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>10k</p>
      </div>

    </div>
  </div>
</footer>
</body>

</html>
