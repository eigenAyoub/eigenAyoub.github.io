<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Kaizen | 10k</title>
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Kaizen" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="10k" />
<meta property="og:description" content="10k" />
<link rel="canonical" href="http://localhost:4000/blogs/on-chip-memory/" />
<meta property="og:url" content="http://localhost:4000/blogs/on-chip-memory/" />
<meta property="og:site_name" content="Kaizen" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Kaizen" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"10k","headline":"Kaizen","url":"http://localhost:4000/blogs/on-chip-memory/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Kaizen" /></head>

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Kaizen</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <p><strong>Content:</strong></p>

<ul>
  <li><a href="">Occupancy of an SM.</a></li>
  <li><a href="">Arithmetic intensity.</a></li>
  <li><a href="">CUDA memory types.</a></li>
</ul>

<h2 id="occupancy-of-an-sm">Occupancy of an SM:</h2>

<p>First know the specs of your GPU (e.g., an A100, <a href="https://github.com/eigenAyoub/cuda-linear-alg/props.cu">Code</a>):</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cc <span class="nv">$ </span>nvcc <span class="nt">-o</span> props props.cu  <span class="o">&amp;&amp;</span> ./props
Number of CUDA devices: 1
Device 0: NVIDIA A100 80GB PCIe
Compute capability: 8.0
Total global memory: 79.1384 GB
Shared memory per block: 48 KB
Shared memory per SM: 164 KB
Registers per block: 65536
Warp size: 32
Max threads per block: 1024
Max threads per SM: 2048
Max threads dimensions: 1024 x 1024 x 64
Max grid size: 2147483647 x 65535 x 65535
Clock rate: 1410000
Total constant memory: 65536
Texture alignment: 512
Multiprocessor count <span class="o">(</span><span class="c">#SMs): 108</span>
</code></pre></div></div>

<p>The occupancy per SM is defined as follows: \(\frac{\# \text{active warps per SM}}{\text{max # of warps per SM}}\).</p>

<p>For instance, if I choose a block size of 32 threads, I would have 1024 threads per SM (32 blocks per SM, each has 32 threads), which yields an Occupancy of 1/2, as the maximum number of threads for an A100 is 2048 (64 warps per SM).</p>

<p><strong>Notice</strong> how the Shared memory per SM <code class="language-plaintext highlighter-rouge">IS NOT EQUAL</code> to [{shared memory per Block} x {#max number of blocks per SM}]. If your block is using too much shared memory, then it would limit the number of blocks assigned to an SM.</p>

<h2 id="performance-bounds-and-arithmetic-intensity">Performance bounds and arithmetic intensity:</h2>

<p><strong>Bounds or bottlenecks:</strong></p>

<p>The question you ask: Why can’t your program can’t run faster? Two answers:</p>
<ol>
  <li>If only my GPU can do more operations/second -&gt; <strong>Compute bound</strong>.
    <ul>
      <li>Data transfer is not an issue. There is just not enough compute.</li>
    </ul>
  </li>
  <li>If only my GPU can move more data/second -&gt; <strong>Memory bound</strong>.
    <ul>
      <li>Some compute cores might be idle, waiting for data.</li>
    </ul>
  </li>
</ol>

<p><strong>Arithmetic intensity:</strong></p>

<p>The arithmetic (or computational) intensity is defined as the ratio of floating point operations <strong>to</strong> bytes accessed from the global memory. For instance, it’s equal to 0.25 flops/byte in the following example.</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">width</span><span class="p">;</span> <span class="n">k</span><span class="o">++</span><span class="p">){</span>
	<span class="n">Pvalue</span> <span class="o">+=</span> <span class="n">M</span><span class="p">[</span><span class="n">row</span><span class="o">*</span><span class="n">Width</span><span class="o">+</span><span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">N</span><span class="p">[</span><span class="n">k</span><span class="o">*</span><span class="n">width</span> <span class="o">+</span> <span class="n">col</span><span class="p">]</span>
<span class="p">}</span>
</code></pre></div></div>

<p>An A100, DRAM peak bandwidth is 1555 GB/second, hence for the simple example above, we can barely do 389 GFLOPS (Giga Flops per second). This accounts to only 2% of the (theoretical) peak single-precision operations throughput ~ 19.5k GFLOPS. The peak operation throughput is equivalent to performing 12 ops per second.</p>

<p><strong>Desired compute intensity.</strong></p>

<p>Consider we have the following specs:</p>

<ul>
  <li><strong>GPU FLOPs</strong>: 14 TFLOP/s (14,000 GFLOP/s)</li>
  <li><strong>Memory Bandwidth</strong>: 900 GB/s</li>
</ul>

<p>A desired compute-to-memory-access ratio is:</p>

\[\frac{14{,}000 \,\text{GFLOPS}}{900 \,\text{GB/s}} \approx 15.6 \,\text{FLOP/byte}.\]

<p><span>⚠</span> <strong>Important &gt;</strong> Each single-precision floating-point operation (FLOP) operates on <strong>4 bytes</strong> (32 bits) of data. Thus:</p>

\[\text{FLOP/byte} \times 4 \rightarrow 15.6 \times 4 \approx 62 \text{Per Floating point access}\]

<!--## The roofline model:

* The question: Is your program compute-bound or memory-bound.
* I am too dumb, always struggle to understand this.

## Tiling, and how it improves the arithmetic intensity:

* Naive matrix multiplication has a ration of `0.25 FP / B` (embarassing).



## CUDA memory types:  // move this section to a separate file.

![Memory](/src/media-gpu/mem.png) 


The Global memory (DRAM) and the Constant memory share the same access latency. The host can W and R on both, while the device can only R the constant memory. 

The Local memory is placed in Global memory, and has the same latency.
* BUT, it is not shared between threads, every thread has it's own region.
* Every thread places elements that can't be placed in their own registers:
	* Statically allocated arrays.
	* Spilled registers. 
	* Elements of the thread's call stack. (like what, wdym?)

Registers and shared memory are on-chip memories.

* Registers are allocated to individual threads.
* A kernel typically uses registers to allocate frequently (and privately) accessed variables to each thread. 
* Shared memory is allocated to thread blocks, and shared between threads on the same block. Usually used to share data, and cooperate between threads, and share intermediate results. 



Differences between registers, shared memory and global memory:

* Global memory is implemented with DRAM technology (long access latency, low access bandwidth). 
* Registers: very short access latency, and drastically higher access bandwidth compared to the global memory.



Each access to registers involves fewer instructions than an access to the global memory. 

The scope and lifetime of each variable:
### Registers, how different are they used in CPU Vs GPU:
-->

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>
  <div class="wrapper">
    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">
              Kaizen
          </li>
        </ul>
      </div>
      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/eigenAyoub"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">eigenAyoub</span></a></li><li><a href="https://instagram.com/curl.ayoub"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#instagram"></use></svg> <span class="username">curl.ayoub</span></a></li><li><a href="https://www.linkedin.com/in/benayad"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg> <span class="username">benayad</span></a></li><li><a href="https://www.twitter.com/benayad_"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">benayad_</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>10k</p>
      </div>

    </div>
  </div>
</footer>
</body>

</html>
