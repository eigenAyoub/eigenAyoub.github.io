\documentclass{article}

\usepackage{hyperref}
\usepackage{tcolorbox}

% this is supposed to the `` of md.
\newtcbox{\cls}{on line,boxrule=0pt,boxsep=0pt,colback=lightgray,top=1pt,bottom=1pt,left=1pt,right=1pt,arc=0pt,fontupper=\ttfamily}

\title{Kronecker Products and GPT2}
\author{Ayoub @ Uni Passau}

\begin{document}

\maketitle

\begin{abstract}

	GPT2 checkpoints from HuggingFace reach a approximate loss of 3.11 on OpenWebText. 	In this work, we factorize the main parameters of GPT2, as Kronecker Products leading to a compression factor of $x\%$, and ask the following question:  What is the fastest way to reach back to the 3.11 loss using the Kronecker factors. And how to optimally manipulate training and distillation of the small compressed network. We particularly investigate freezing (for both training and distillation), and smooth transitioning (explained later on).


\end{abstract}

\begin{itemize}
	\item \textbf{freezing} the pre-trained weights, helps boost accuracy.
	\item One by One distillation: drop the weights slowly and distill through the network.
\end{itemize}

\textbf{Methodology:}
\begin{enumerate}
	\item Train GPT2 to a certain level. -- $3.11$.
	\item Decompose the MLP weights.
	\item Try a bunch of methods and see which one guarantees getting back to the original loss as fast as possible.
\end{enumerate}

\tableofcontents
\newpage

\section{Resutls}%
\label{sec:Resutls}

Results so far on wikitext103, wikitext2 and lambada. 

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
 & & \multicolumn{3}{c|}{Datasets} \\ \hline
\# Params &  Model            & wikitext-103 & wikitext-2 & Lambada \\ \hline
124M      & GPT2              & 29.16        & 24.67      & 45.28      \\ \hline
82M       & DistilGPT2        & 44.53        & 36.48      & 76.00      \\ \hline
81M       & \textbf{KronyPT-81M-1350}  & 41.98        & 34.99      & -          \\ \hline
81M       & \textbf{KronyPT-81M-3950}  & -            & -          & 64.92      \\ \hline
81M       & TQCompressedGPT2  & 40.28        & 32.25      & 64.72      \\ \hline
81M       & KnGPT-2 (Huawei)  & 40.97        & 32.81      & 67.62      \\ \hline
\end{tabular}
\caption{Comparison of different models on various datasets.}
\end{table}



\section{Introduction and Setup}
\label{sec:Introduction and Setup}

% add the latest paper

A lot of work has been done on the compression of LLMs using factorization methods, including Kronecker Products. Most notably for encoder based based, and rarely applied to decoder based methods (\cite{tahaei2022kroneckerbert}, \cite{edalati2021kroneckr}).

\begin{comment}
To the best of our knowledge, not many have focused on Freezing the initial pre-trained weights, i.e., most methods drop newly factorized matrices into the already pre-trained parameters, without carefully blending, nor assessing/studying how much the model is relying on already trained weights.

In this work:

\begin{itemize}
	\item   We investigate the impact of freezing on the quality of learning,  and also demonstrate the capacity for the network to just not rely on the new weights, and just use the already pre-trained one. 
	\item We also introduce a new distillation / training mechanism to ease the entry of these factorized weights into the old architecture. 
	\item We also show that freezing the other weights, helps the new introduced get more activation.
\end{itemize}

In this work, we demonstrate that without freezing, the network relies more on the pre-trained weights (to a varying degree, depending on how the new proposed weights are initialized). And with freezing, newly introduced weights learn faster.

Usually the newly introduced weights are brute forced into training, and in many cases not, (knowing how many LLMs are under-trained + many residual connections), the pre-trained carry the weights. We also compare the gradient activations in both cases (when we free and when don't).

In this work we demonstrate the effectiveness of freezing, and of slow integration of weights, either through training or distillation.


Using Van Loan has no remarkable benefit improvement over $50\%$ pruning.




\subsection{Learning rate, batch size, and gradient accumulation}%
\label{sub:lr}

In this section, we discuss the impact of the \textbf{learning rate}, \textbf{batch size} and \textbf{gradient accumulation} on the training trajectory,  namely on the first $10000$ iterations of training.


> Note: Higher

%%Insert figure here.

Based on these results, we follow the  
\begin{itemize}
	\item Learning rate: cosine schedule, with a warm-up of 500 tiers.
	\item Gradient accumulation: 3 across 4 nodes (a good balance between 2 and 5)
	\item Batch size: 12, better stability. 
\end{itemize}


This configuration allows a balanced trade-off between speed of updates and stochasticity of the loss with each iterations. And it also allows multiple experiments to be run simultaneously on a node of 4 A100s. Reframe this: you lose a bit of performance, but you gain a lot of memory to try out other approach at the same time, sounds silly, but this is important when you're GPU poor.

So, with this config, and with only 100k steps, avereging approx 0.5% data. 

What can we do better? distillation?

\subsection{Distillation}%
\label{sub:Distillation}

In this section, we inherit the checkpoint from the section before, and use GPT2 as a teacher network for a classic Inter Layer distillation. We try two methods. 

Result: It is not working. Just supervise it.

\subsection{A subsection Freezing maybe?}%
\label{sub:lr}

We believe that not freezing the original pre-trained weight, is eventually detrimental to overall training, and the largest the model is, the more the network relies on already pre-trained parameters. This is tracked using gradient checking. When we freeze the weights. To validate our hypothesis, we track gradient activation (also known as the attention matrix, not to be confused with attention matrix of transformer models, and notice that, when we freeze then fine-tune, we notice better gradient activation.)

	grad.sum for :
		1. resuming training for all params for 3k steps.
		2. freezing for 2k, train all for 1k
		3. 1 by 1 training with freezing (at each step, direct flow the flow to origin gpt2) for 2k, the tran all for 1k

	same logic applies in 

We test some interesting properties of distillation. We first position ourselves in a small setup, and then study how these observations scale to larger models. 



\end{comment}


We study how far we can push the compression ratio, and then impact of down-scaling on the quality of the model. We also investigate adding multiple Kronecker factors.


\section{Training setups}%
\label{sec:Training setups}

Here we discuss the main. After many I am going for the main, mostly  




\section{Kronecker Decomposition}%
\label{sub:Kronecker Decomposition}

In this section, we study different Kronecker decomposition setups, and the percentage of compression it would  lead to. So far we only decompose the weights \cls{c\_fc.weight} and \cls{c\_proj.weight} (each has $2.3M$ in the original GPT2-small architecture.). Each transformer layer (there are $12$ in total) has two of these weights. They count to $56.6M$ in total ($45\%$ of GPT2 $124M$). Hence any significant reduction to these matrices would lead to a remarkable compression ratio of the whole model. We choose not to compress the other weights, namely, attention weights and embedding matrix for various reasons that we will expose later on. 

%of the attention for various reasons, first, it is showed that they're pretty dense (change this, and add more refs.), and second for implementation purposes (Flash Attention is too good!).
\subsection{The 95M model}%
\label{sub:The 95M model}
The most basic strategy is to divide one of the dimensions of each W by 2, this would lead to a 95M model. The parameter \cls{$p_1 = $ c\_fc.weight} (resp. \cls{$p_2 = $ c\_proj.weight}) has a shape of $(3072, 768)$ (resp.  $(768, 3072)$). We first try the following decomposition: $p_1 = \underbrace{W_{11}}_{\text{(3072,384)}} \otimes \underbrace{W_{12}}_{\text{(1,2)}}$  and $p_2 = \underbrace{W_{21}}_{\text{(384,3072)}} \otimes \underbrace{W_{22}}_{\text{(2,1)}}$  


This decomposition would lead to to reduction of $28M$ ($50\%$). The new network would have approx $95M$. Our goal is to eventually reach the $82M$ mark, similar to DistilGPT2, and other Factorized models (inserts other refs here).

\subsection{Different decomposition schemes:}%
\label{sec:Different decomposition schemes}

It is reasonable to aim for a decomposition that guarantees the maximum rank we can get. Since the Rank of Kronecker products is multiplicative, i.e., $\mathrm{rank}(A\otimes B) = \mathrm{rank}(A)\cdot\mathrm{rank}(B)$ \footnote{\href{ https://math.stackexchange.com/questions/707091/elementary-proof-for-textrank-lefta-otimes-b-right-textranka-cdot}{Link to proof: https://math.stackexchange.com/questions/707091/elementary-proof-for-textrank-lefta-otimes-b-right-textranka-cdot}}, we can easily compute the rank of each possible decomposition. In our case, we have $W \in \mathbb{R}^{(m,n)}$ where $m = 3072$ and $n = 768$. Hence, for each layer of GPT2, we aim to find the ``optimal'' $A \in \mathbb{R}^{(m_1, n_1)}$, and $B\in \mathbb{R}^{(m_2, n_2)$, i.e.,: 

	\[W \approx A\otimes B, \qquad m = m_1 m_2, n = n_1 n_2 \]. 

	W.l.o.g, for each decomposition $(A,B)$ the maximum rank we can reach is $\min(m_1,n_1) \cdot \min(m_2,n_2)$. And each of the reduced decompositions would have exactly $m_1 n_1 + m_2 n_2$ parameters. Hence, theoritically, the maximum rank we can get is $384$ of a $(3072, 768)$ matrix. The following table summarizes all possible combinations, alongside the reduction it would lead to per layer, and the total number of parameters in GPT2, for only those decompostions of rank $384$.
\section{Impact of down-scaling: How small can we go}%
\label{sub:get-down}

In this section, we study the impact of down-scaling on the compressed models, we train 4 different architectures, with variant dimensions:

\begin{itemize}
	\item 67M: diverges // try with higher batch size (the least we can get)
	\item 81M: Super.
	\item 81M: with a Variant scheme
	\item 95M: duh duh duh. (the highest we can get)
\end{itemize}

Keep in mind:
\begin{itemize}
	\item You can probably stabilize training using various tricks.  It's just an endless loop. We are not going to play the What-IF game. One single config, that's it.
	\item One could try to have a mixed strategy:
		\begin{itemize}
			\item  higher levels of compression in the early layers.
			\item  higher levels of compression in the late layers.
			\item  Every odd layer 
			\item  In the middle 
		\end{itemize}
			\item 
			\item 
		\end{itemize}
\end{itemize}	



\subsection{Initialization}%
\label{sub:Initialization}

Here we test a new initialization trick based on pruning for the 95M models. And compare it to the VL method. Pruning shows a much better

\subsection{The fuck is Van Loan}%
\label{sub:The fuck is Van Loan}
XX
\subsection{Pruning based Initialization}%
\label{sub:Pruning based Initialization}
XX

\subsection{Questions}%
\label{sub:Questions}


\begin{itemize}
	\item When we plug new KP matrices to the pre-trained model, is it useful to freeze the other weights that are already pre-trained? Or using different learning rates schemes maybe?

	\item When don't don't freeze the weights. Does the network rely on other pre-trained methods?
	\begin{itemize}
		\item **The idea that I'm challenging here**: A lot of work on NNs compression using factorized methods,
			    claim that the network only need to be (post-)trained on small % of the original training steps/data.                                                                   
		\item But, what the fail to mention or elaborate on at least, is that not the weights matrices of the network are decomposed.                                                 
		\item And with all the residual connection that are present in most attention networks,                                                                                       
			    one could suspect, that maybe the weights that were not decomposed are taking over...                                                                                   
		\item A good remedy for this is to freeze the original weights during the post-training, and only allow the new dropped in (factorized matrices) to be updated with backprop. 
			   
	\end{itemize}
	\item This strategy could also be implemented in distillation. We refer to this method as **forced distillation.**
	\item When we freeze, we investigate how useful is it to distill matrices one by one, rather than drop in the matrices all at once. And also investigate if the order of dropping has any significance, bottom up or top bottom...

\end{itemize}

\bibliography{bib}
\bibliographystyle{ieeetr}


\end{document}
