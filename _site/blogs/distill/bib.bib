@article{edalati2021kroneckr,
  title={Kronecker decomposition for gpt compression},
  author={Edalati, Ali and Tahaei, Marzieh and Rashid, Ahmad and Nia, Vahid Partovi and Clark, James J and Rezagholizadeh, Mehdi},
  journal={arXiv preprint arXiv:2110.08152},
  year={2021}
}


@inproceedings{tahaei2022kroneckerbert,
  title={KroneckerBERT: Significant Compression of Pre-trained Language Models Through Kronecker Decomposition and Knowledge Distillation},
  author={Tahaei, Marzieh and Charlaix, Ella and Nia, Vahid and Ghodsi, Ali and Rezagholizadeh, Mehdi},
  booktitle={Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={2116--2127},
  year={2022}
}

@article{abronin2024tqcompressor,
  title={TQCompressor: improving tensor decomposition methods in neural networks via permutations},
  author={Abronin, V and Naumov, A and Mazur, D and Bystrov, D and Tsarova, K and Melnikov, Ar and Oseledets, I and Dolgov, S and Brasher, R and Perelshtein, M},
  journal={arXiv preprint arXiv:2401.16367},
  year={2024}
}


@article{paperno2016lambada,
  title={The LAMBADA dataset: Word prediction requiring a broad discourse context},
  author={Paperno, Denis and Kruszewski, Germ{\'a}n and Lazaridou, Angeliki and Pham, Quan Ngoc and Bernardi, Raffaella and Pezzelle, Sandro and Baroni, Marco and Boleda, Gemma and Fern{\'a}ndez, Raquel},
  journal={arXiv preprint arXiv:1606.06031},
  year={2016}
}


@article{merity2016pointer,
  title={Pointer sentinel mixture models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  journal={arXiv preprint arXiv:1609.07843},
  year={2016}
}

@inproceedings{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  booktitle={NeurIPS EMC^2 Workshop},
  year={2019}
}

@article{hoffmann2022training,
  title={Training compute-optimal large language models. arXiv},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and de Las Casas, Diego and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022},
  publisher={Retrieved 2023-01-02, from http://arxiv. org/abs/2203.15556}
}

@misc{Gokaslan2019OpenWeb,  
	title={OpenWebText Corpus},
	author={Aaron Gokaslan and Vanya Cohen},
	howpublished={\url{http://Skylion007.github.io/OpenWebTextCorpus}}, 
	year={2019}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@misc{branwen2021scaling,
	  title={The scaling hypothesis},
	    author={Branwen, Gwern},
		  year={2021}
}



@article{vaswani2017attention,
	  title={Attention is all you need},
	    author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
		  journal={Advances in neural information processing systems},
		    volume={30},
			  year={2017}
}



@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}


@article{reid2024gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Reid, Machel and Savinov, Nikolay and Teplyashin, Denis and Lepikhin, Dmitry and Lillicrap, Timothy and Alayrac, Jean-baptiste and Soricut, Radu and Lazaridou, Angeliki and Firat, Orhan and Schrittwieser, Julian and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}

@misc{claude3,
  author = {Anthropic},
  title={The Claude 3 Model Family: Opus, Sonnet, Haiku}
  year = {2024},
  howpublished = {\url{https://www.anthropic.com/news/claude-3-family}},
}


@misc{llama,
  author = {Meta},
  title={Llama 3}
  year = {2024},
  howpublished = {\url{https://llama.meta.com/llama3/}},
}

@article{llama3,
	title={Llama 3 Model Card},
	author={AI@Meta},
	year={2024},
	url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}
}

@book{van1993approximation,
  title={Approximation with Kronecker products},
  author={Van Loan, Charles F and Pitsianis, Nikos},
  year={1993},
  publisher={Springer}
}


@inproceedings{wu2016compression,
  title={Compression of fully-connected layer in neural network by kronecker product},
  author={Wu, Jia-Nan},
  booktitle={2016 Eighth International Conference on Advanced Computational Intelligence (ICACI)},
  pages={173--179},
  year={2016},
  organization={IEEE}
}


