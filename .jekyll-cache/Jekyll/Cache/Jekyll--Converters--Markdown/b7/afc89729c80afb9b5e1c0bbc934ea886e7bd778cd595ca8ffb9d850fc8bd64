I"≥<p>Keywords: RL, Q-learning, stochastic approximation.</p>

\[p(s',r \lvert s,a)=\Pr \{S_t=s',R_t=r \lvert S_{t-1}=s,A_{t-1}=a\}\]

<h2 id="proof-of-convergence-of-q-learning">Proof of convergence of Q-Learning</h2>

<p>In this blogpost, I‚Äôll write about what I have learned regarding the asymptomatic guarentees of Q-Learning and TD-Learning. Eventhough we are going to use some results from Stochastic Approximation (SA) theory without a proof, going through this material made me appriciate Q-learning way more than just a toy tool.</p>

<p>It is also fair to meantion that the ‚Äúasymtotic‚Äù convergence of Q-Learning is fairly *EASY* compared to finding guanrentees on the * RATE *\ of convergence, which is still an active field of research, I guess! Props to you RL theory folks!</p>

<p>At first I‚Äôll formulate the problem setting in a <em>SA framework</em>, state the conditions and assumptions, and  finally I‚Äôll the main theorem to the case of Q-learning.</p>

<h2 id="content">Content:</h2>
<ul>
  <li><a href="#setting">Setting</a></li>
  <li><a href="#algorithm">Algorithm</a></li>
  <li><a href="#proof">Proof</a>
    <ul>
      <li><a href="#sketch">Sketch</a></li>
      <li><a href="#proof">Proof</a></li>
    </ul>
  </li>
  <li><a href="#references">References</a></li>
</ul>

<h2 id="stochastic-approximation-of-a-contractive-operator">Stochastic Approximation of a contractive operator:</h2>

<p>Let \(L: R^d \longrightarrow R^d\) be a \(c\)-lipschitz function where \(c&lt;1\) (This obviously implies contraction, which implies the fixed point theorem in Banach Spaces). We are interested in finding  \(\theta^*\) the fixed point of \(L\), i.e., \(L \theta^* = \theta^*\).</p>

<blockquote>
  <p>Bear in mind that the Q-learning actor that would assume the role of \(L\) is the Bellman Optimality Operator \(T^*\). As you know the \(T^*\) is a contraction mapping for the infinite? ‚Äî check this out today</p>
</blockquote>

<blockquote>
  <p>While \(\theta_{t}\) would be replaced by \(Q_t(.,.)\), therefore, the dimension \(d\) of \(\theta_{t}\) should match the state-action space cardinal \(\lvert X x A \lvert\).</p>
</blockquote>

<p>With the right conditions on \(\{ \alpha_{t} \}_{t=&gt;0}\) (Known as the Monro ‚Ä¶ SA conditions 1 below, kn), having access to \(L\) guarantees us to land in \(\theta^*\) using the following (smooth) interative update:</p>

\[\theta_{t+1} = (1-\alpha_t) \theta_{t} + \alpha_t L \theta_{t}\]

<p>SA conditions:</p>
<ol>
  <li>Divergence of  \(\{ \alpha_{t} \}_{t=&gt;0}\)</li>
  <li>Convergence of  \(\{ \alpha_{t}^2 \}_{t=&gt;0}\)</li>
</ol>

<p>Intuitively speaking, we the want ‚Ä¶ to slowly diverge.</p>

<p>The first condition ensures the mean, the second is crutial to make the variance of the estimate vanish to 0.</p>

<p>The past result is a straight application of the fixed point theorem in Banach Spaces, for \(L' = (1- \alpha)I + \alpha L\).</p>

<p>Having access to \(L\) is a luxary we can‚Äôt generally afford in online settings, where the model dynamics are agnostic. A more realistic situation is to only have access to a noisy *unbiased*\ estimate, namely \(L \theta_{t} +  \eta_{t}\) with \(\mathbb{E}[\eta_{t}] = 0\).</p>

<p>To make it even more interesting, we want to update each componenent apart, \(\theta_{t}^i\) for \(i \partof [1:d]\) as follows:</p>

<table>
  <tbody>
    <tr>
      <td>\(\alpha_{j}\) = 0 for $$ j</td>
      <td>= i $$   [#]</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>With our analogy in mind, in Q-Learning, we don‚Äôt update Q_t synchronously for all state-action pair.</p>
</blockquote>

<p>Therefore, we want to prove that the sequence of \(\theta_{t}\) generated by  procedure above would eventully converge to the fixed point? the answer is YES [x-ref], provided some assumptions of course. Nothing comes for free in this business! Namely o the noise \(\etha\)</p>

<p>Let‚Äôs call them Assuption A1 following [1] notations:</p>

<p>Assumption A1:
1- Regarding the mean:
This assuption makes the estimate unbiased</p>

<p>2 Regarding the variance:</p>

<p>Now let‚Äôs state the theorem: Convergence of stochastce approximation:</p>

<p>Usually, the conditions in A2, especially the iones regarding the variance are the ‚Äúhardest‚Äù to prove. The other ones are usually obvious. The operator would usually be T_star which a contraction w.r.t the inf_norm and the {alpha_t} sastifies the SA conditions by design.</p>

<p>Let‚Äôs see in Q-Learning for instance. The update rule looks like:</p>

<hr />

<p>When re-formulated:</p>

<hr />

<p>N_t = ‚Ä¶.</p>

<p>we get ‚Ä¶</p>

<p><strong>Remark How mayn times do we need to visit a pair \(\( X,A \)\)</strong></p>

<p>A condition that we never stated regarding how we explore the state-action space. As the \alpha_{t} needs to be verified for each parin \((X,A)\), this requires us to visit each state infinelty many‚Ä¶ otherwise the first condition wouldn‚Äôt be verified.</p>

<p>Of course this is ony an asymptotic guaarntee.</p>

<h2 id="setting">Setting:</h2>
<p>We have an MDPs \(\)
Take it from Preux Papers, Check 2 or 3 papers</p>

<h2 id="algorithm">Algorithm:</h2>

<p>The interaction between the agent and its environment is captured through an MDP, a 5-tuple &lt;X,A, P, R, gamma&gt;. WE aim to learn an Optimal Policy by first learning the Optimal Action-Value function, Q^* for each pair x,a in X x A.</p>

<p>The asynchronous Q-Learning is as follows:</p>

<p>Note that this is an off-policy RL algorithm, as the behavioral p[olicy differes fro the one we are trying evaluaatin (\pi_{g}(Q_t)</p>

<h2 id="proof">Proof:</h2>
<p>\(\begin{aligned}
V(s) &amp;= \mathbb{E}[G_t \vert S_t = s] \\
&amp;= \mathbb{E} [R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots \vert S_t = s] \\
&amp;= \mathbb{E} [R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \dots) \vert S_t = s] \\
&amp;= \mathbb{E} [R_{t+1} + \gamma G_{t+1} \vert S_t = s] \\
&amp;= \mathbb{E} [R_{t+1} + \gamma V(S_{t+1}) \vert S_t = s]
\end{aligned}\)</p>

<h3 id="sketch">Sketch:</h3>

<h3 id="proof-1">Proof:</h3>
<p>Let this and that be ‚Ä¶</p>

<h2 id="references">References:</h2>
<ul>
  <li>[] Neuro dynamic programming</li>
  <li>[] Link to TD-learnig paper</li>
  <li>[] Amir massoud Fahramad</li>
  <li>[] Link to youtube playlist, github linl</li>
</ul>

:ET