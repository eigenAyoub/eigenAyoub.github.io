I"Ω<p><strong>Abstract:</strong></p>

<p>In this tutorial blog, we‚Äôll prove the correctness of the method animated above in a general setting. Elaborate on how we can use it to sample from the multi-variate Gaussian (MVN). Finally, we will discuss a practical approach and illustrate it with some code snippets from the Scipy and Numpy.</p>

<p><strong>Outline:</strong></p>

<ol>
  <li><a href="#1">Correctness: The transformation Law</a>
    <ol>
      <li><a href="#11">Generating the exponential law from the uniform distribution</a></li>
    </ol>
  </li>
  <li><a href="#2">MVN case: Normal x Normal = Normal</a></li>
  <li><a href="#3">Practical implementation:</a>
    <ol>
      <li><a href="#31">Cholsky decomposition</a></li>
      <li><a href="#32">Open Source</a></li>
    </ol>
  </li>
</ol>

<hr />

<p><strong>Notation:</strong></p>

<p>If \(X\) is a random variable (RV), we‚Äôll use \(F_X\)  to denote its commutative distribution function (CDF) and \(f_X\) for its density function.</p>

<hr />
<h3 id="1-correctness-">**1. Correctness **<a name="1"></a></h3>

<p>Put in simple terms, if \(F_N^{-1}\) is the reverse of the CDF \(F_N\) of the standard normal distribution and \(U\) is a uniformely distributed RV on the interval \([0,1]\). The animation above claims that \(F_N^{-1}(U)\) is normally distributed.</p>

<p><strong>Proof:</strong></p>

<p>Let‚Äôs define the RV \(Y\) as  \(Y = F_N^{-1}(U)\)  and let‚Äôs prove that \(Y = N(0,1)\)</p>

<p>For \(y\) in \(R\) :</p>

\[\begin{align*}
   F_Y(y) &amp;= P(Y&lt;y) \\
&amp;= P(F_N^{-1}(U) &lt; y) \\
&amp;= P(U &lt; F_N(y)) \\
&amp;= F_N(y)
\end{align*}\]

<p>Where the third equality follows from the fact that \(F_N^{-1}\) is strictly increasing on the range \([0,1]\)</p>

<p>Since the CDF completely determines a distribution, we conclude that \(Y\) is equal to \(N\), which completes our proof.</p>

<h1 id="anything-below-is-to-do">Anything below is TO-DO:</h1>

<table>
  <tbody>
    <tr>
      <td>Note:  This actually is a simple version of the transformation law, i.e., when \(Y = g(X)\) s.t \(g\) is strictly monotonic and differentiallable  on the range of \(X\)  , we can prove the following $$F_Y(y)  = f_X(g^{-1}(y))</td>
      <td>\frac{d g^{-1} }{dy} (y)</td>
      <td>$$</td>
    </tr>
  </tbody>
</table>

<p>In this case, \(X\) is the uniform distribution and \(g\) is the inverse CDF of the normal distribution.</p>

<h4 id="11-example-the-exponential-law-"><strong>1.1. Example: The exponential law:</strong> <a name="11"></a></h4>
<p>This example is very recurrent, mainly because of how important this distribution is and how we can analytically (and simply) get the CDF \(F_{Exp}\) and its inverse \(F_{Exp}^{-1}\) .</p>

<p>We have \(F_X(x) = 1- e^{-\lambda x}\)  and   \(F_X^{-1}=\)</p>

<p>And following the same approach proved above, the RV \(X = F_{Exp}^{-1}(U)\) is definitely going to be  an exponential distribution.</p>

<hr />
<h3 id="2-multi-variate-case"><strong>2. Multi-variate case:</strong><a name="2"></a></h3>

<p><strong>Fancy Gaussian Features:</strong></p>

<p>The Gaussian distribution is known for many features. The ‚Äúfanciest‚Äù ones to cite Including being the distribution that maximises the entropy for a given mean and variance, and namely how it weirdly appears in many asymptotic behaviours of the sums of i.i.d  RVs.</p>

<p><strong>Practical reasons:</strong></p>

<p>The key reason why we keep using the normal distribution is its very convenient mathematical properties, including:</p>

<ul>
  <li>An affine transformation of a Gaussian distribution is still Gaussian</li>
  <li>Product of many Gaussian is Gaussian</li>
  <li>Sub-sample of Gaussian is Gaussian</li>
  <li>Conditional Gaussian are Gaussian ‚Ä¶</li>
</ul>

<p><strong>Sampling from \(N(0_{R^{N}},I_N)\)  :</strong></p>

<p>The product property let us conclude that sampling \(N\) one-dimensional  \(X_i ~ N(0,1)\) is actually th same as sampling  an \(N\)-dimensional RV \(X = [X_1, .., X_N] ~ N(0_{R^{N}},I_N)\)</p>

<p>**Sampling from \(N(Nu,Sigma)\) **</p>

<hr />
<h3 id="3-practical-implementation-"><strong>3. Practical implementation:</strong> <a name="3"></a></h3>

<h4 id="31-the-cholseky-decomposition-"><strong>3.1. The Cholseky decomposition:</strong> <a name="31"></a></h4>
<p>One of the most famous decompositions in linear algbra is the Cholsky decompostion. In fact it is one of the most useful techniques in ML practice. Not only is it used for sam</p>

<p>The cholsky decompostion of a matrxi should be thought intuitively as the square root of a matrix</p>

<p>By the way, I‚Äôll devote a tutorial blog for the Cholsky decomposition and dive deeper in the theory and application of it. You can find it here when It‚Äôs done:</p>

<h4 id="32-illustration-from-scipy-and-numpy"><strong>3.2. Illustration from Scipy and Numpy:</strong><a name="32"></a></h4>

<hr />

<p><strong>Some useful resources:</strong></p>

<ul>
  <li>Some serious</li>
  <li>Philthinglip Hennig leklek erkejr</li>
  <li>Nando de Freitas course on</li>
</ul>

:ET