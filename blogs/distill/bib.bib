@article{edalati2021kroneckr,
  title={Kronecker decomposition for gpt compression},
  author={Edalati, Ali and Tahaei, Marzieh and Rashid, Ahmad and Nia, Vahid Partovi and Clark, James J and Rezagholizadeh, Mehdi},
  journal={arXiv preprint arXiv:2110.08152},
  year={2021}
}


@inproceedings{tahaei2022kroneckerbert,
  title={KroneckerBERT: Significant Compression of Pre-trained Language Models Through Kronecker Decomposition and Knowledge Distillation},
  author={Tahaei, Marzieh and Charlaix, Ella and Nia, Vahid and Ghodsi, Ali and Rezagholizadeh, Mehdi},
  booktitle={Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={2116--2127},
  year={2022}
}

@article{abronin2024tqcompressor,
  title={TQCompressor: improving tensor decomposition methods in neural networks via permutations},
  author={Abronin, V and Naumov, A and Mazur, D and Bystrov, D and Tsarova, K and Melnikov, Ar and Oseledets, I and Dolgov, S and Brasher, R and Perelshtein, M},
  journal={arXiv preprint arXiv:2401.16367},
  year={2024}
}


@article{paperno2016lambada,
  title={The LAMBADA dataset: Word prediction requiring a broad discourse context},
  author={Paperno, Denis and Kruszewski, Germ{\'a}n and Lazaridou, Angeliki and Pham, Quan Ngoc and Bernardi, Raffaella and Pezzelle, Sandro and Baroni, Marco and Boleda, Gemma and Fern{\'a}ndez, Raquel},
  journal={arXiv preprint arXiv:1606.06031},
  year={2016}
}


@article{merity2016pointer,
  title={Pointer sentinel mixture models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  journal={arXiv preprint arXiv:1609.07843},
  year={2016}
}

@inproceedings{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  booktitle={NeurIPS EMC^2 Workshop},
  year={2019}
}


