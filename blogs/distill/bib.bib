@article{edalati2021kroneckr,
  title={Kronecker decomposition for gpt compression},
  author={Edalati, Ali and Tahaei, Marzieh and Rashid, Ahmad and Nia, Vahid Partovi and Clark, James J and Rezagholizadeh, Mehdi},
  journal={arXiv preprint arXiv:2110.08152},
  year={2021}
}


@inproceedings{tahaei2022kroneckerbert,
  title={KroneckerBERT: Significant Compression of Pre-trained Language Models Through Kronecker Decomposition and Knowledge Distillation},
  author={Tahaei, Marzieh and Charlaix, Ella and Nia, Vahid and Ghodsi, Ali and Rezagholizadeh, Mehdi},
  booktitle={Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={2116--2127},
  year={2022}
}

@article{abronin2024tqcompressor,
  title={TQCompressor: improving tensor decomposition methods in neural networks via permutations},
  author={Abronin, V and Naumov, A and Mazur, D and Bystrov, D and Tsarova, K and Melnikov, Ar and Oseledets, I and Dolgov, S and Brasher, R and Perelshtein, M},
  journal={arXiv preprint arXiv:2401.16367},
  year={2024}
}


@article{paperno2016lambada,
  title={The LAMBADA dataset: Word prediction requiring a broad discourse context},
  author={Paperno, Denis and Kruszewski, Germ{\'a}n and Lazaridou, Angeliki and Pham, Quan Ngoc and Bernardi, Raffaella and Pezzelle, Sandro and Baroni, Marco and Boleda, Gemma and Fern{\'a}ndez, Raquel},
  journal={arXiv preprint arXiv:1606.06031},
  year={2016}
}


@article{merity2016pointer,
  title={Pointer sentinel mixture models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  journal={arXiv preprint arXiv:1609.07843},
  year={2016}
}

@inproceedings{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  booktitle={NeurIPS EMC^2 Workshop},
  year={2019}
}

@article{hoffmann2022training,
  title={Training compute-optimal large language models. arXiv},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and de Las Casas, Diego and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022},
  publisher={Retrieved 2023-01-02, from http://arxiv. org/abs/2203.15556}
}

@misc{Gokaslan2019OpenWeb,  
	title={OpenWebText Corpus},
	author={Aaron Gokaslan and Vanya Cohen},
	howpublished={\url{http://Skylion007.github.io/OpenWebTextCorpus}}, 
	year={2019}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}
