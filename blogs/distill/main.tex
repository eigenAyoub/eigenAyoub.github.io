\documentclass{article}

\usepackage{hyperref}
\usepackage{tcolorbox}
\usepackage{amsmath}

% this is supposed to the `` of md.
\newtcbox{\cls}{on line,boxrule=0pt,boxsep=0pt,colback=lightgray,top=1pt,bottom=1pt,left=1pt,right=1pt,arc=0pt,fontupper=\ttfamily}

\title{Kronecker Products and GPT2}
\author{Ayoub @ Uni Passau}

\begin{document}

\maketitle

\begin{abstract}

What do we do differently: \textbf{We} have empirical evidence not to use distillation, as experiments show better results with just Vanilla supervised training. \textbf{We} don't compress the attention matrices matrices, and focus mainly on the MLPs (why?, see below). \textbf{We} try different compression ratios and see how far we can push down the size. \textbf{We} add multiple Kronecker Factors. \textbf{Finally}, we introduce a new (and efficient, as it doesn't require any extra compute) initialization trick based on pruning.

\begin{center}
\textbf{Some false narratives of the other papers}
\begin{itemize}
	\item \textbf{Parameter count}: It appears like the competitor paper do not count the output matrix, and I quote ``Note that number of parameters of the models are
reported excluding the output embedding layer in language modelling which is not compressed, is
equal to row Parameters''. If this is true, then their model would technically be a 120M. The reason why we (and GPT2 paper) don't count the output matrix is because it is identical to the embedding matrix, this known as weight tying or weight sharing. Which does not seem the case with the other papers (I have contacted the other paper for clarifications through github).

	\item  ``\textbf{We only use 3\% of the data}": both papers claim that they only used 3\% of the training data to train their models. We argue that in this set up, limiting your training data to only a fraction  does not imply a better compression/factorization method, for the simple and following reasons:
		\begin{enumerate}
			\item They inherit almost half the weights from the old GPT2 checkpoint that was trained for numerous epochs, and has seen the whole data. 
			\item They use Van Loan method, hence, even, when they don't match the original checkpoint, VL is still a smart init, some knowledge is definitely passed through the SVD. 
			\item You literally use the output matrix of a pre-trained GPT2.
		\end{enumerate}
\end{itemize}

Hence, we don't know ``how much knowledge" has been passed through the already trained parameters. A fair comparison would be to initialize \textbf{all the parameters} of the new compressed model with random values, and not rely on any of the other pre-trained ones. Which is clearly not the case here.


\end{abstract}

\newpage

\tableofcontents
\newpage

\section{Resutls:}%
\label{sec:Resutls}

Early results on wikitext103, wikitext2 \cite{merity2016pointer} and Lambada \cite{paperno2016lambada}. \textbf{KronyPT} is our model, and the suffixes $1350$ and $3950$ refer to different checkpoints. Both models are already outperforming distilGPT2 (\cite{sanh2019distilbert}) on the 3 datasets. And the $3950$ checkpoint is outperforming \textbf{KnGPT2} (\cite{tahaei2022kroneckerbert}) on Lambada, while slightly still behind \textbf{TQCompressedGPT2}  \cite{abronin2024tqcompressor}. We measure perplexity (duh) using the Hugging Face interface that was used by the other papers.

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
 & & \multicolumn{3}{c|}{Datasets} \\ \hline
\# Params &  Model            & wikitext-103 & wikitext-2 & Lambada \\ \hline
124M      & GPT2              & 29.16        & 24.67      & 45.28      \\ \hline
82M       & DistilGPT2        & 44.53        & 36.48      & 76.00      \\ \hline
81M       & \textbf{KronyPT-81M-1350}  & \textbf{41.98}   & \textbf{34.99}      & -          \\ \hline
81M       & \textbf{KronyPT-81M-3950}  & -            	  & -          & \textbf{64.92}      \\ \hline
\end{tabular}
\caption{Perplexity results of Krony-PT and DistilGPT}
\end{table}


\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
 & & \multicolumn{3}{c|}{Datasets} \\ \hline
\# Params &  Model            & wikitext-103 & wikitext-2 & Lambada \\ \hline
81M       & \textbf{KronyPT-81M-1350}  & 41.98        & 34.99      & -          \\ \hline
81M       & \textbf{KronyPT-81M-3950}  & -            & -          & 64.92      \\ \hline
119M       & TQCompressedGPT2  & 40.28        & 32.25      & 64.72      \\ \hline
119M       & KnGPT-2 (Huawei)  & 40.97        & 32.81      & 67.62      \\ \hline
\end{tabular}
\caption{Perplexity of Krony-PT against other Kronecker based models.}
\end{table}


\section{Introduction}
\label{sec:Introduction}

% add the latest paper

A lot of work has been done on the compression of LLMs using factorization methods, including Kronecker Products. Most notably for encoder based based, and rarely applied to decoder based methods (\cite{tahaei2022kroneckerbert}, \cite{edalati2021kroneckr}, \cite{abronin2024tqcompressor}).


We study how far we can push the compression ratio, and record then impact of down-scaling on the quality of the model. We also investigate adding multiple Kronecker factors.

\begin{comment}
To the best of our knowledge, not many have focused on Freezing the initial pre-trained weights, i.e., most methods drop newly factorized matrices into the already pre-trained parameters, without carefully blending, nor assessing/studying how much the model is relying on already trained weights.

In this work:

\begin{itemize}
	\item   We investigate the impact of freezing on the quality of learning,  and also demonstrate the capacity for the network to just not rely on the new weights, and just use the already pre-trained one. 
	\item We also introduce a new distillation / training mechanism to ease the entry of these factorized weights into the old architecture. 
	\item We also show that freezing the other weights, helps the new introduced get more activation.
\end{itemize}

In this work, we demonstrate that without freezing, the network relies more on the pre-trained weights (to a varying degree, depending on how the new proposed weights are initialized). And with freezing, newly introduced weights learn faster.

Usually the newly introduced weights are brute forced into training, and in many cases not, (knowing how many LLMs are under-trained + many residual connections), the pre-trained carry the weights. We also compare the gradient activations in both cases (when we free and when don't).

In this work we demonstrate the effectiveness of freezing, and of slow integration of weights, either through training or distillation.


Using Van Loan has no remarkable benefit improvement over $50\%$ pruning.

\subsection{Learning rate, batch size, and gradient accumulation}%
\label{sub:lr}

In this section, we discuss the impact of the \textbf{learning rate}, \textbf{batch size} and \textbf{gradient accumulation} on the training trajectory,  namely on the first $10000$ iterations of training.


> Note: Higher

%%Insert figure here.

Based on these results, we follow the  
\begin{itemize}
	\item Learning rate: cosine schedule, with a warm-up of 500 tiers.
	\item Gradient accumulation: 3 across 4 nodes (a good balance between 2 and 5)
	\item Batch size: 12, better stability. 
\end{itemize}


This configuration allows a balanced trade-off between speed of updates and stochasticity of the loss with each iterations. And it also allows multiple experiments to be run simultaneously on a node of 4 A100s. Reframe this: you lose a bit of performance, but you gain a lot of memory to try out other approach at the same time, sounds silly, but this is important when you're GPU poor.

So, with this config, and with only 100k steps, avereging approx 0.5% data. 

What can we do better? distillation?

\subsection{Distillation}%
\label{sub:Distillation}

In this section, we inherit the checkpoint from the section before, and use GPT2 as a teacher network for a classic Inter Layer distillation. We try two methods. 

Result: It is not working. Just supervise it.

\subsection{A subsection Freezing maybe?}%
\label{sub:lr}

We believe that not freezing the original pre-trained weight, is eventually detrimental to overall training, and the largest the model is, the more the network relies on already pre-trained parameters. This is tracked using gradient checking. When we freeze the weights. To validate our hypothesis, we track gradient activation (also known as the attention matrix, not to be confused with attention matrix of transformer models, and notice that, when we freeze then fine-tune, we notice better gradient activation.)

	grad.sum for :
		1. resuming training for all params for 3k steps.
		2. freezing for 2k, train all for 1k
		3. 1 by 1 training with freezing (at each step, direct flow the flow to origin gpt2) for 2k, the tran all for 1k

	same logic applies in 

We test some interesting properties of distillation. We first position ourselves in a small setup, and then study how these observations scale to larger models. 



\end{comment}




\section{Training Set-up}%
\label{sec:Training setups}

Add here the basic set-up that I used to train the latest models, basically Chinchilla recommendations. (for later)

Some key differences:
\begin{itemize}
	\item \textbf{Batch size:} We find better and more stable loss curve with higher batch sizes, a typical step would need 6 batches (gradient acc.) of 12 samples. 
	\item \textbf{Learning rate:} We found better, and more stable results when using a constant learning rate of $6e-4$. (check this again), The other set-up is to use a cosine scheduler, peaking at $6e04$ after a warm-up of 500 steps?.  Learning rate
\end{itemize}

\section{Kronecker Decomposition}%
\label{sub:Kronecker Decomposition}

In this section, we study different Kronecker decomposition setups, and the percentage of compression it would  lead to. So far we only decompose the weights \cls{c\_fc.weight} and \cls{c\_proj.weight} (each has $2.3M$ in the original GPT2-small architecture.). Each transformer layer (there are $12$ in total) has two of these weights. They count to $56.6M$ in total ($45\%$ of GPT2 $124M$). Hence any significant reduction to these matrices would lead to a remarkable compression ratio of the whole model. We choose not to compress the other weights, namely, attention weights and embedding matrix for various reasons that we will expose later on. 

%of the attention for various reasons, first, it is showed that they're pretty dense (change this, and add more refs.), and second for implementation purposes (Flash Attention is too good!).
\subsection{The 95M model}%
\label{sub:The 95M model}
The most basic strategy is to divide one of the dimensions of each W by 2, this would lead to a 95M model. The parameter \cls{$p_1 = $ c\_fc.weight} (resp. \cls{$p_2 = $ c\_proj.weight}) has a shape of $(3072, 768)$ (resp.  $(768, 3072)$). We first try the following decomposition: $p_1 = \underbrace{W_{11}}_{\text{(3072,384)}} \otimes \underbrace{W_{12}}_{\text{(1,2)}}$  and $p_2 = \underbrace{W_{21}}_{\text{(384,3072)}} \otimes \underbrace{W_{22}}_{\text{(2,1)}}$  


This decomposition would lead to to reduction of $28M$ ($50\%$). The new network would have approx $95M$. Our goal is to eventually reach the $82M$ mark, similar to DistilGPT2, and other Factorized models (inserts other refs here).

\subsection{Different decomposition schemes:}%
\label{sec:Different decomposition schemes}

It is reasonable to aim for a decomposition that guarantees the maximum rank we can get. Since the Rank of Kronecker products is multiplicative, i.e., $\mathrm{rank}(A\otimes B) = \mathrm{rank}(A)\cdot\mathrm{rank}(B)$ \footnote{\href{ https://math.stackexchange.com/questions/707091/elementary-proof-for-textrank-lefta-otimes-b-right-textranka-cdot}{Link to proof: https://math.stackexchange.com/questions/707091/elementary-proof-for-textrank-lefta-otimes-b-right-textranka-cdot}}, we can easily compute the rank of each possible decomposition. In our case, we have $W \in \mathbb{R}^{(m,n)}$ where $m = 3072$ and $n = 768$. Hence, for each layer of GPT2, we aim to find the ``optimal'' $A \in \mathbb{R}^{(m_1, n_1)}$, and $B\in \mathbb{R}^{(m_2, n_2)$, i.e.,: 

	\[W \approx A\otimes B, \qquad m = m_1 m_2, n = n_1 n_2 \]. 

	W.l.o.g, for each decomposition $(A,B)$ the maximum rank we can reach is $\min(m_1,n_1) \cdot \min(m_2,n_2)$. And each of the reduced decompositions would have exactly $m_1 n_1 + m_2 n_2$ parameters. Hence, theoritically, the maximum rank we can get is $768$ of a $(3072, 768)$ matrix. The following table summarizes some possible combinations, alongside the reduction it would lead to per layer, and the total number of parameters in GPT2, for only those decompostions of maximal attainable rank. We are particularly interested in 3 class of models, the 67M, the 81M and the 96M. (Need to add this) Furthermore, we add multiple factors to the models labeled with \textbf{MF} (second table / a few decompositions are missing, check this out. e.g., 3072, 384).
\begin{table}[htb!]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
Name 	& Dimension 		& 	#params 	& Model size\\
\hline
67M		&  (64, 32)         &   3200       & 67,893,504  \\ \hline 
		&  (64, 48)         &   3840       & 67,908,864  \\ \hline 
		&  (96, 32)         &   3840       & 67,908,864  \\ \hline
		&  (64, 64)         &   4672       & 67,928,832  \\ \hline
		&  (128, 32)        &   4672       & 67,928,832  \\ \hline 
		&  (96, 48)         &   5120       & 67,939,584  \\ \hline 
		&  (96, 64)         &   6528       & 67,973,376  \\ \hline
		&  (128, 48)        &   6528       & 67,973,376  \\ \hline
		&  (128, 64)        &   8480       & 68,020,224  \\ \hline 
		&  (96, 96)         &   9472       & 68,044,032  \\ \hline 
 		&  (192, 48)        &   9472       & 68,044,032  \\ \hline
		&  (128, 96)        &   12480      & 68,116,224  \\ \hline
		&  (192, 64)        &   12480      & 68,116,224  \\ \hline 
		&  (128, 128)       &   16528      & 68,213,376  \\ \hline 
MF1		&  (256, 64)        &   16528      & 68,213,376  \\ \hline
		&  \cdots &   \cdots    & \cdots \\ \hline
MF2		&  (1024, 256)      &   262153     & 74,108,376  \\ \hline 
		&  (768, 384)       &   294920     & 74,894,784  \\ \hline
		&  (1024, 384)      &   393222     & 77,254,032  \\ \hline
81M 	&  (768, 768)       &   589828     & 81,972,576  \\ \hline
		&  (1536, 384)      &   589828     & 81,972,576  \\ \hline
		&  (1024, 768)      &   786435     & 86,691,144  \\ \hline
96M		&  (1536, 768)      &   1179650    & 96,128,304  \\ \hline
GPT2	&  (3072, 768)      &   2359297    & 124,439,832 \\ \hline
\end{tabular}
\caption{Different compression schemes}
\end{table}



\begin{table}[htb!]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Name 	& Dimension 		& 	#params    & 1 factor    & 2 factors   & 3 factors     \\ \hline                                                                                 
MF1		&  (256, 64)        &   16528      & 68,2M       & 68,6  & 69M    \\ \hline
MF2		&  (1024, 256)      &   262153     & 74M         & 80M   & 86M    \\ \hline 
\end{tabular}
\caption{Adding multiple Kronecker Factors}
\end{table}


\newpage
\section{How small can we go?}%
\label{sub:get-down}

In this section, we study the impact of down-scaling on the compressed models, we train 4 different architectures, with variant dimensions:

\begin{itemize}
	\item 67M: diverges // try with higher batch size (the least we can get)
	\item 81M: Super.
	\item 81M: with a Variant scheme (67M with multiple factors / 1025-256 with  two factors)
	\item 95M: duh duh duh. (the highest we can get)
\end{itemize}

Keep in mind:
\begin{itemize}
	\item You can probably stabilize training using various tricks.  It's just an endless loop. We are not going to play the What-IF game. One single config, that's it.
	\item One could try to have a mixed strategy:
		\begin{itemize}
			\item  higher levels of compression in the early layers.
			\item  higher levels of compression in the late layers.
			\item  Every odd layer 
			\item  In the middle 
		\end{itemize}
		\end{itemize}
\end{itemize}	


\section{Initialization}%
\label{sub:Initialization}

Since we inherit a GPT2 checkpoint that was trained for multiple epochs on the Open Web Text (OWT) (cite here), we want to initiliaze our weights in a way that leverages the old pre-training as much as possible. This is of course obvious for the parameters that are common between \textbf{GPT2} and \textbf{KronyPT} (i.e., we match).  But more tricky for the weights that are decomposed into Kronecker Factors. In our work, we try two different approaches, Van Loan decomposition (cite here), and a Pruning based method exclusively for the 95M model.

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline

 & & \multicolumn{3}{c|}{Datasets} \\ \hline
\# Params &  Model            & wikitext-103 & wikitext-2 & Lambada \\ \hline
95M       & \textbf{Krony-PT1}  & 41.80        & 35.50      & 61.34         \\ \hline
95M       & \textbf{Krony-PT1}  & 41.81        & 36.02      & 59.95         \\ \hline
\end{tabular}
\caption{Comparison of different models on various datasets.}
\end{table}
\subsection{Van Loan Method}%
\label{sub:The fuck is Van Loan}

XX (maybe not even write this section)

\subsection{Pruning based Initialization}%
\label{sub:Pruning based Initialization}


We propose a new initialization strategy by inducing sparsity in the first factor of the Kronecker Product, and prune it by half. This is equivalent to picking the second factor as 
\begin{bmatrix}
1 \\
0
\end{bmatrix}. Now, we illustrate how this procedure works with a random matrix.

\[
\begin{bmatrix}
a_{11} & a_{12} & a_{13} & a_{14} & a_{15} \\
a_{21} & a_{22} & a_{23} & a_{24} & a_{25} \\
a_{31} & a_{32} & a_{33} & a_{34} & a_{35} \\
a_{41} & a_{42} & a_{43} & a_{44} & a_{45} \\
a_{51} & a_{52} & a_{53} & a_{54} & a_{55} \\
a_{61} & a_{62} & a_{63} & a_{64} & a_{65} \\
\end{bmatrix}
\xrightarrow[\text{pruning}]{}
\begin{bmatrix}
a_{11} & a_{12} & a_{13} & a_{14} & a_{15} \\
0 & 0 & 0 & 0 & 0  \\
a_{31} & a_{32} & a_{33} & a_{34} & a_{35} \\
0 & 0 & 0 & 0 & 0  \\
a_{51} & a_{52} & a_{53} & a_{54} & a_{55} \\
0 & 0 & 0 & 0 & 0  \\
a_{61} & a_{62} & a_{63} & a_{64} & a_{65} \\
\end{bmatrix}
=
\begin{bmatrix}
a_{11} & a_{12} & a_{13} & a_{14} & a_{15} \\
a_{31} & a_{32} & a_{33} & a_{34} & a_{35} \\
a_{51} & a_{52} & a_{53} & a_{54} & a_{55} \\
a_{61} & a_{62} & a_{63} & a_{64} & a_{65} \\
\end{bmatrix}
\otimes
\begin{bmatrix}
1  \\
0  
% Elements of matrix D
\end{bmatrix}
 

\]







\begin{comment}
	
\subsection{Questions}%
\label{sub:Questions}


\begin{itemize}
	\item When we plug new KP matrices to the pre-trained model, is it useful to freeze the other weights that are already pre-trained? Or using different learning rates schemes maybe?

	\item When don't don't freeze the weights. Does the network rely on other pre-trained methods?
	\begin{itemize}
		\item **The idea that I'm challenging here**: A lot of work on NNs compression using factorized methods,
			    claim that the network only need to be (post-)trained on small % of the original training steps/data.                                                                   
		\item But, what the fail to mention or elaborate on at least, is that not the weights matrices of the network are decomposed.                                                 
		\item And with all the residual connection that are present in most attention networks,                                                                                       
			    one could suspect, that maybe the weights that were not decomposed are taking over...                                                                                   
		\item A good remedy for this is to freeze the original weights during the post-training, and only allow the new dropped in (factorized matrices) to be updated with backprop. 
			   
	\end{itemize}
	\item This strategy could also be implemented in distillation. We refer to this method as **forced distillation.**
	\item When we freeze, we investigate how useful is it to distill matrices one by one, rather than drop in the matrices all at once. And also investigate if the order of dropping has any significance, bottom up or top bottom...

\end{itemize}
\end{comment}

\newpage
\bibliography{bib}
\bibliographystyle{ieeetr}


\end{document}
