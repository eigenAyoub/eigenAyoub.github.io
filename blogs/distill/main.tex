\documentclass{article}

\usepackage{hyperref}
\usepackage{tcolorbox}
\usepackage{amsmath}

% this is supposed to the `` of md.
\newtcbox{\cls}{on line,boxrule=0pt,boxsep=0pt,colback=lightgray,top=1pt,bottom=1pt,left=1pt,right=1pt,arc=0pt,fontupper=\ttfamily}

\title{Kronecker Products and GPT2}
\author{Ayoub @ Uni Passau}

\begin{document}

\maketitle

\begin{abstract}

LLMs are cool, everywhere, but are pretty big.

\newpage

\tableofcontents
\newpage



\section{Introduction}
\label{sec:Introduction}

% add the latest paper

A lot of work has been done on the compression of LLMs using factorization methods, including Kronecker Products. Most notably for encoder based based, and rarely applied to decoder based methods (\cite{tahaei2022kroneckerbert}, \cite{edalati2021kroneckr}, \cite{abronin2024tqcompressor}).


We study how far we can push the compression ratio, and record then impact of down-scaling on the quality of the model. We also investigate adding multiple Kronecker factors.

\begin{comment}
To the best of our knowledge, not many have focused on Freezing the initial pre-trained weights, i.e., most methods drop newly factorized matrices into the already pre-trained parameters, without carefully blending, nor assessing/studying how much the model is relying on already trained weights.

In this work:

\begin{itemize}
	\item   We investigate the impact of freezing on the quality of learning,  and also demonstrate the capacity for the network to just not rely on the new weights, and just use the already pre-trained one. 
	\item We also introduce a new distillation / training mechanism to ease the entry of these factorized weights into the old architecture. 
	\item We also show that freezing the other weights, helps the new introduced get more activation.
\end{itemize}

In this work, we demonstrate that without freezing, the network relies more on the pre-trained weights (to a varying degree, depending on how the new proposed weights are initialized). And with freezing, newly introduced weights learn faster.

Usually the newly introduced weights are brute forced into training, and in many cases not, (knowing how many LLMs are under-trained + many residual connections), the pre-trained carry the weights. We also compare the gradient activations in both cases (when we free and when don't).

In this work we demonstrate the effectiveness of freezing, and of slow integration of weights, either through training or distillation.

Using Van Loan has no remarkable benefit improvement over $50\%$ pruning.


Based on these results, we follow the  
\begin{itemize}
	\item Learning rate: cosine schedule, with a warm-up of 500 tiers.
	\item Gradient accumulation: 3 across 4 nodes (a good balance between 2 and 5)
	\item Batch size: 12, better stability. 
\end{itemize}

This configuration allows a balanced trade-off between speed of updates and stochasticity of the loss with each iterations. And it also allows multiple experiments to be run simultaneously on a node of 4 A100s. Reframe this: you lose a bit of performance, but you gain a lot of memory to try out other approach at the same time, sounds silly, but this is important when you're GPU poor.

\end{comment}

\textbf{Our contributions} (so far, delete this) could be summarized as follows:
\begin{itemize}
    \item We use weight tying, i.e., the embedding and softmax matrices are identical. This makes our model, the only "effective" 81M model compared to (X,Y).
    \item We have a systematic compression scheme, i.e., compress all layers the same way. We don't see any reasons to why we would only compress odd layers (besides to "force" the number of parameters to be under certain threshold).
    \item We try different compression schemes (67M, 81M, and 95M)
		\begin{itemize}
			\item We propose a new simple initialization for the 95M model.
			\item We use multiple factors for the VL based inits.
		\end{itemize}
    \item We don't use distillation (empirical evidence shows no benefits compared to only Vanilla supervised learning).
\end{itemize}
\section{Methodology}%
\label{sec:Training setups}

\subsection{Kronecker Decomposition}%
\label{sub:Kronecker Decomposition}

In this section, we study different Kronecker decomposition setups, and the percentage of compression it would  lead to. So far we only decompose the weights \cls{c\_fc.weight} and \cls{c\_proj.weight} (each has $2.3M$ in the original GPT2-small architecture.). Each transformer layer (there are $12$ in total) has two of these weights. They count to $56.6M$ in total ($45\%$ of GPT2 $124M$). Hence any significant reduction to these matrices would lead to a remarkable compression ratio of the whole model. We choose not to compress the other weights, namely, attention weights and embedding matrix for various reasons that we will expose later on. 

%of the attention for various reasons, first, it is showed that they're pretty dense (change this, and add more refs.), and second for implementation purposes (Flash Attention is too good!).
\subsection{The 95M model}%
\label{sub:The 95M model}
The most basic strategy is to divide one of the dimensions of each W by 2, this would lead to a 95M model. The parameter \cls{$p_1 = $ c\_fc.weight} (resp. \cls{$p_2 = $ c\_proj.weight}) has a shape of $(3072, 768)$ (resp.  $(768, 3072)$). We first try the following decomposition: $p_1 = \underbrace{W_{11}}_{\text{(3072,384)}} \otimes \underbrace{W_{12}}_{\text{(1,2)}}$  and $p_2 = \underbrace{W_{21}}_{\text{(384,3072)}} \otimes \underbrace{W_{22}}_{\text{(2,1)}}$  


This decomposition would lead to to reduction of $28M$ ($50\%$). The new network would have approx $95M$. Our goal is to eventually reach the $82M$ mark, similar to DistilGPT2, and other Factorized models (inserts other refs here).

\subsection{Different decomposition schemes:}%
\label{sec:Different decomposition schemes}

It is reasonable to aim for a decomposition that guarantees the maximum rank we can get. Since the Rank of Kronecker products is multiplicative, i.e., $\mathrm{rank}(A\otimes B) = \mathrm{rank}(A)\cdot\mathrm{rank}(B)$ \footnote{\href{ https://math.stackexchange.com/questions/707091/elementary-proof-for-textrank-lefta-otimes-b-right-textranka-cdot}{Link to proof: https://math.stackexchange.com/questions/707091/elementary-proof-for-textrank-lefta-otimes-b-right-textranka-cdot}}, we can easily compute the rank of each possible decomposition. In our case, we have $W \in \mathbb{R}^{(m,n)}$ where $m = 3072$ and $n = 768$. Hence, for each layer of GPT2, we aim to find the ``optimal'' $A \in \mathbb{R}^{(m_1, n_1)}$, and $B\in \mathbb{R}^{(m_2, n_2)$, i.e.,: 

	\[W \approx A\otimes B, \qquad m = m_1 m_2, n = n_1 n_2 \]. 

	W.l.o.g, for each decomposition $(A,B)$ the maximum rank we can reach is $\min(m_1,n_1) \cdot \min(m_2,n_2)$. And each of the reduced decompositions would have exactly $m_1 n_1 + m_2 n_2$ parameters. Hence, theoritically, the maximum rank we can get is $768$ of a $(3072, 768)$ matrix. The following table summarizes some possible combinations, alongside the reduction it would lead to per layer, and the total number of parameters in GPT2, for only those decompostions of maximal attainable rank. We are particularly interested in 3 class of models, the 67M, the 81M and the 96M. (Need to add this) Furthermore, we add multiple factors to the models labeled with \textbf{MF} (second table / a few decompositions are missing, check this out. e.g., 3072, 384).
\begin{table}[htb!]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
Name 	& Dimension 		& 	#params 	& Model size\\
\hline
67M		&  (64, 32)         &   3200       & 67,893,504  \\ \hline 
		&  (64, 48)         &   3840       & 67,908,864  \\ \hline 
		&  (96, 32)         &   3840       & 67,908,864  \\ \hline
		&  (64, 64)         &   4672       & 67,928,832  \\ \hline
		&  (128, 32)        &   4672       & 67,928,832  \\ \hline 
		&  (96, 48)         &   5120       & 67,939,584  \\ \hline 
		&  (96, 64)         &   6528       & 67,973,376  \\ \hline
		&  (128, 48)        &   6528       & 67,973,376  \\ \hline
		&  (128, 64)        &   8480       & 68,020,224  \\ \hline 
		&  (96, 96)         &   9472       & 68,044,032  \\ \hline 
 		&  (192, 48)        &   9472       & 68,044,032  \\ \hline
		&  (128, 96)        &   12480      & 68,116,224  \\ \hline
		&  (192, 64)        &   12480      & 68,116,224  \\ \hline 
		&  (128, 128)       &   16528      & 68,213,376  \\ \hline 
MF1		&  (256, 64)        &   16528      & 68,213,376  \\ \hline
		&  \cdots &   \cdots    & \cdots \\ \hline
MF2		&  (1024, 256)      &   262153     & 74,108,376  \\ \hline 
		&  (768, 384)       &   294920     & 74,894,784  \\ \hline
		&  (1024, 384)      &   393222     & 77,254,032  \\ \hline
81M 	&  (768, 768)       &   589828     & 81,972,576  \\ \hline
		&  (1536, 384)      &   589828     & 81,972,576  \\ \hline
		&  (1024, 768)      &   786435     & 86,691,144  \\ \hline
96M		&  (1536, 768)      &   1179650    & 96,128,304  \\ \hline
GPT2	&  (3072, 768)      &   2359297    & 124,439,832 \\ \hline
\end{tabular}
\caption{Different compression schemes}
\end{table}


\begin{table}[htb!]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Name 	& Dimension 		& 	#params    & 1 factor    & 2 factors   & 3 factors     \\ \hline                                                                                 
MF1		&  (256, 64)        &   16528      & 68,2M       & 68,6  & 69M    \\ \hline
MF2		&  (1024, 256)      &   262153     & 74M         & 80M   & 86M    \\ \hline 
\end{tabular}
\caption{Adding multiple Kronecker Factors}
\end{table}


\newpage

\subsection{Initialization}%
\label{sub:Initialization}

Since we inherit a GPT2 checkpoint that was trained for multiple epochs on the Open Web Text (OWT) (cite here), we want to initiliaze our weights in a way that leverages the old pre-training as much as possible. This is of course obvious for the parameters that are common between \textbf{GPT2} and \textbf{KronyPT} (i.e., we match).  But more tricky for the weights that are decomposed into Kronecker Factors. In our work, we try two different approaches, Van Loan decomposition (cite here), and a Pruning based method exclusively for the 95M model.

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline

 & & \multicolumn{3}{c|}{Datasets} \\ \hline
\# Params &  Model            & wikitext-103 & wikitext-2 & Lambada \\ \hline
95M       & \textbf{Krony-PT1}  & 41.80        & 35.50      & 61.34         \\ \hline
95M       & \textbf{Krony-PT1}  & 41.81        & 36.02      & 59.95         \\ \hline
\end{tabular}
\caption{Comparison of different models on various datasets.}
\end{table}
\subsubsection{Van Loan Method}%
\label{sub:The fuck is Van Loan}

XX (maybe not even write this section)

\subsubsection{Pruning based Initialization}%
\label{sub:Pruning based Initialization}


We propose a new initialization strategy by inducing sparsity in the first factor of the Kronecker Product, and prune it by half. This is equivalent to picking the second factor as 
\begin{bmatrix}
1 \\
0
\end{bmatrix}. Now, we illustrate how this procedure works with a random matrix.

\[
\begin{bmatrix}
a_{11} & a_{12} & a_{13} & a_{14} & a_{15} \\
a_{21} & a_{22} & a_{23} & a_{24} & a_{25} \\
a_{31} & a_{32} & a_{33} & a_{34} & a_{35} \\
a_{41} & a_{42} & a_{43} & a_{44} & a_{45} \\
a_{51} & a_{52} & a_{53} & a_{54} & a_{55} \\
a_{61} & a_{62} & a_{63} & a_{64} & a_{65} \\
\end{bmatrix}
\xrightarrow[\text{pruning}]{}
\begin{bmatrix}
a_{11} & a_{12} & a_{13} & a_{14} & a_{15} \\
0 & 0 & 0 & 0 & 0  \\
a_{31} & a_{32} & a_{33} & a_{34} & a_{35} \\
0 & 0 & 0 & 0 & 0  \\
a_{51} & a_{52} & a_{53} & a_{54} & a_{55} \\
0 & 0 & 0 & 0 & 0  \\
a_{61} & a_{62} & a_{63} & a_{64} & a_{65} \\
\end{bmatrix}
=
\begin{bmatrix}
a_{11} & a_{12} & a_{13} & a_{14} & a_{15} \\
a_{31} & a_{32} & a_{33} & a_{34} & a_{35} \\
a_{51} & a_{52} & a_{53} & a_{54} & a_{55} \\
a_{61} & a_{62} & a_{63} & a_{64} & a_{65} \\
\end{bmatrix}
\otimes
\begin{bmatrix}
1  \\
0  
% Elements of matrix D
\end{bmatrix}
 

\]

\section{Experiements}%
\label{sub:Experiements}

\subsection{Training setup}%
\label{sub:Setup}

For pre-training, we follow general industry standards, namely of Chinchilla \cite{hoffmann2022training}. We noticed more stable loss curves with higher \textbf{batch sizes}, typically $128$ batch size (attained with a gradient accumulation of $4$, and each forward pass seeing a batch of $32$), and constant \textbf{learning rate} of $6 \times 10^{-5}$ (better than a cosine scheduler with a warm-up). After pre-training on approximately one epoch, we pick the best checkpoint and train further on $\approx$ 500k tokens per step (as suggested in the literature) which amounts in our set up (sequence length of $1024$) to a batch size of $512$. All experiments have been conducted using a single A100 80 GB, for one epoch of on Open Web Text \cite{Gokaslan2019OpenWeb}, an open source reproduction of OpenAI's WebText dataset.



\subsection{Results}%
\label{sub:results}

We refer to our models with the nomenclature \textbf{Krony-PT-XM\{-Y\}}, with $X$ representing the number of the parameters, and optionaly $Y$ referring to separate checkpoints of the same model class. We namely focus on 2 class models, the  \textbf{Krony-PT-81M} and \textbf{Krony-PT-95M}. We evaluate our models next-token prediction capabilities on 3 datasets, wikitext103, wikitext2 \cite{merity2016pointer} and Lambada \cite{paperno2016lambada}. 

\subsection{The 81M class:}%
\label{sub:The 81M class:}

\textbf{Krony-PT-81M} is our model class of 81M parameters, and the suffixes $1350$ and $3950$ refer to different checkpoints. Both models clearly outperform distilGPT2 (\cite{sanh2019distilbert}) on the 3 datasets, with a noticeable better perplexity on Lambada \cite{paperno2016lambada}.


\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
 & & \multicolumn{3}{c|}{Datasets} \\ \hline
\# Params &  Model            & wikitext-103 & wikitext-2 & Lambada \\ \hline
124M      & GPT2              & 29.16        & 24.67      & 45.28      \\ \hline
82M       & DistilGPT2        & 44.53        & 36.48      & 76.00      \\ \hline
81M       & \textbf{KronyPT-81M-1350}  & \textbf{41.98}   & \textbf{34.99}      & -          \\ \hline
81M       & \textbf{KronyPT-81M-3950}  & -            	  & -          & \textbf{64.92}      \\ \hline
\end{tabular}
\caption{Perplexity results of Krony-PT and DistilGPT}
\end{table}


\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
 & & \multicolumn{3}{c|}{Datasets} \\ \hline
\# Params &  Model            & wikitext-103 & wikitext-2 & Lambada \\ \hline
81M       & \textbf{KronyPT-81M-1350}  & 41.98        & 34.99      & -          \\ \hline
81M       & \textbf{KronyPT-81M-3950}  & -            & -          & 64.92      \\ \hline
119M       & TQCompressedGPT2  & 40.28        & 32.25      & 64.72      \\ \hline
119M       & KnGPT-2   & 40.97        & 32.81      & 67.62      \\ \hline
\end{tabular}
\caption{Perplexity of Krony-PT against other Kronecker based models.}
\end{table}

\subsection{The 95M class:}%
\label{sub:The 95M class:}

\subsection{Scale test}%
\label{sub:Scale test}
How far can we push down the size? tl;dr: 67M does not work!

\subsection{General trends:}%

Things to add here:

\begin{itemize}
	\item Models do not get better at Wiki, but do improve on Lambada.
	\item Distillation is not helpful 
	\item Does pruning help? For training and for inference?
	\item does re-plugging the matrix work? 
\end{itemize}


\section{Discussion}%
\label{sec:Discussion}

We clarify in this section some design choices of the other papers:

\subsection{Parameter count:}%
\label{sub:Parameter count:}

For some reason the other papers do not include the output matrix, and I quote for \cite{ ``Note that number of parameters of the models are
reported excluding the output embedding layer in language modelling which is not compressed, is
equal to row Parameters''. If this is true, then their model would technically be a 120M. The reason why we (and GPT2 paper) don't count the output matrix is because it is identical to the embedding matrix, this known as weight tying or weight sharing. Which does not seem the case with the other papers (I have contacted the other paper for clarifications through github).

\subsection{Only 3\% of data}%
\label{sub:Only 3\% of data}

both papers claim that they only used 3\% of the training data to train their models. We argue that in this set up, limiting your training data to only a fraction  does not imply a better compression/factorization method, for the simple and following reasons:

\begin{enumerate}
	\item They inherit almost half the weights from the old GPT2 checkpoint that was trained for numerous epochs, and has seen the whole data. 
	\item They use Van Loan method, hence, even, when they don't match the original checkpoint, VL is still a smart init, some knowledge is definitely passed through the SVD. 
	\item You literally use the output matrix of a pre-trained GPT2.
\end{enumerate}

Hence, we don't know ``how much knowledge" has been passed through the already trained parameters. A fair comparison would be to initialize \textbf{all the parameters} of the new compressed model with random values, and not rely on any of the other pre-trained ones. Which is clearly not the case here.



\begin{comment}
	
\subsection{Questions}%
\label{sub:Questions}


\begin{itemize}
	\item When we plug new KP matrices to the pre-trained model, is it useful to freeze the other weights that are already pre-trained? Or using different learning rates schemes maybe?

	\item When don't don't freeze the weights. Does the network rely on other pre-trained methods?
	\begin{itemize}
		\item **The idea that I'm challenging here**: A lot of work on NNs compression using factorized methods,
			    claim that the network only need to be (post-)trained on small % of the original training steps/data.                                                                   
		\item But, what the fail to mention or elaborate on at least, is that not the weights matrices of the network are decomposed.                                                 
		\item And with all the residual connection that are present in most attention networks,                                                                                       
			    one could suspect, that maybe the weights that were not decomposed are taking over...                                                                                   
		\item A good remedy for this is to freeze the original weights during the post-training, and only allow the new dropped in (factorized matrices) to be updated with backprop. 
			   
	\end{itemize}
	\item This strategy could also be implemented in distillation. We refer to this method as **forced distillation.**
	\item When we freeze, we investigate how useful is it to distill matrices one by one, rather than drop in the matrices all at once. And also investigate if the order of dropping has any significance, bottom up or top bottom...

\end{itemize}
\end{comment}

\newpage
\bibliography{bib}
\bibliographystyle{ieeetr}


\end{document}
