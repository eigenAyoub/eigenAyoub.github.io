\documentclass{article}

\usepackage{hyperref}
\usepackage{tcolorbox}
\usepackage{amsmath}
\usepackage{verbatim}

% this is supposed to the `` of md.
\newtcbox{\cls}{on line,boxrule=0pt,boxsep=0pt,colback=lightgray,top=1pt,bottom=1pt,left=1pt,right=1pt,arc=0pt,fontupper=\ttfamily}

\title{Kronecker Products and GPT2}
\author{Ayoub @ Uni Passau}

\begin{document}

\maketitle

\begin{abstract}
	We introduce Krony-PT, a compression technique of GPT2 \cite{radford2019language} based on Kronecker Products. We specifically target the MLP layers of each transformer layer, and systematically compress the feed forward layer matrices to various degrees. We use optimal Van Loan decompositions to initialize the new matrices, and also introduce a new pruning-based initialization trick. Our method compresses the original GPT2 124M parameters to various smaller models, 67M being the smallest, and 95M being the largest compressed model. Our 81M model variant outperforms distilgpt2 on next-token prediction on all standard language modeling datasets, and shows competitive scores or performs on par with other Kronecker Products based compressed models of GPT2 that are significantly higher in size. 
\end{abstract}

\newpage

\tableofcontents
\newpage

\section{Introduction}
\label{sec:Introduction}

With (/ Given) their rapid and unprecedented development, Large Language Models (LLMs) are revolutionizing many industries and changing the way we perceive and interact with machines. Their strong ability to understand, generate, and perhaps even ``generalize" in numerous domains including automated text generation, language translation, and notably coding, has led many to believe that scaling up models is all you need to achieve Artificial General Intelligence (AGI) (i.e., The Scaling Hypothesis \cite{branwen2021scaling}). Despite their advanced features, the core of LLMs still fundamentally relies on the initial Transformer architecture \cite{vaswani2017attention}. And specifically the encoder only variants that was first introduced with GPT2 \cite{radford2019language}. 

Although GPT-4 \cite{achiam2023gpt} has fairly led the race in the past year, a lot of the big actors of the industry seem to have recently caught up (e.g., Anthropic \cite{claude3}, Meta \cite{llama}, \cite{llama3} , and Google \cite{reid2024gemini}), as demonstrated by the independent and open LMSys Leaderboard \footnote{\href{https://chat.lmsys.org/?leaderboard}{https://chat.lmsys.org/?leaderboard}}. Subsequently, many believe that the two main ingredients are \textbf{compute} and \textbf{data}. And while this seems affordable for big corporations, it poses significant challenges for individuals and smaller organizations to reproduce these SATA models. Hence, advancing work on the compression and factorization methods of LLMs is essential to make these powerful tools more accessible and sustainable for broader use, and a necessity for deployment on edge-devices.

\textbf{Our contributions} could be summarized as follows:
\begin{itemize}
    \item We use weight tying, i.e., the embedding and softmax matrices are identical. This makes our model, the only ``effective" 81M model compared to (X,Y). Which significantly reduces the model size by 38M compared to other methods (see appendix for details regarding parameter count).
    \item We have a systematic compression scheme, i.e., compress all layers the same way. We don't see any reasons to why we would only compress odd layers (besides to "force" the number of parameters to be under certain threshold).
    \item We try different compression schemes (67M, 81M, and 95M)
		\begin{itemize}
			\item We propose a new simple initialization for the 95M model.
			\item We use multiple factors for the VL based units.
			\item We introduce scalers to each factor, which (apparently) helps with generalizing (why? the performance on OWT seems to be ok, but the ppl is  better on other benchmarks, e.g., wiki and lamabada)

		\end{itemize}
    \item We don't use distillation (empirical evidence shows no benefits compared to only Vanilla supervised learning).
\end{itemize}

\section{Related work}%
\label{sec:Related work}

Compressing Large Language Models is currently a major focus in research. In this context, Matrix Factorization provide the perfect framework for compression. There are three major research directions in the compression of LLMs, \textbf{Distillation}, \textbf{Quantization}, and finally \textbf{Factorization}, which is the focus of our work. Generally speaking, Matrix Factorization techniques aim to combine (e.g., a product) lower rank matrices to reconstruct an original (typically bigger) matrix. A lot of work has been done on the compression of LLMs using factorization methods, including Kronecker Products. Most notably for encoder based based, and rarely applied to decoder based methods (\cite{tahaei2022kroneckerbert}, \cite{edalati2021kroneckr}, \cite{abronin2024tqcompressor}).



Motivated by how dense the attention layers are \cite{}, we opted not to compress the QKV matrices.  






\section{Methodology}%
\label{sec:Training setups}

\subsection{Kronecker Decomposition}%
\label{sub:Kronecker Decomposition}

In this section, we study different Kronecker decomposition setups, and the percentage of compression it would  lead to. So far we only decompose the weights \cls{c\_fc.weight} and \cls{c\_proj.weight} (each has $2.3M$ in the original GPT2-small architecture.). Each transformer layer (there are $12$ in total) has two of these weights. They count to $56.6M$ in total ($45\%$ of GPT2 $124M$). Hence any significant reduction to these matrices would lead to a remarkable compression ratio of the whole model. We choose not to compress the other weights, namely, attention weights and embedding matrix for various reasons that we will expose later on. 

%of the attention for various reasons, first, it is showed that they're pretty dense (change this, and add more refs.), and second for implementation purposes (Flash Attention is too good!).
\subsection{The 95M model}%
\label{sub:The 95M model}
The most basic strategy is to divide one of the dimensions of each W by 2, this would lead to a 95M model. The parameter \cls{$p_1 = $ c\_fc.weight} (resp. \cls{$p_2 = $ c\_proj.weight}) has a shape of $(3072, 768)$ (resp.  $(768, 3072)$). We first try the following decomposition: $p_1 = \underbrace{W_{11}}_{\text{(3072,384)}} \otimes \underbrace{W_{12}}_{\text{(1,2)}}$  and $p_2 = \underbrace{W_{21}}_{\text{(384,3072)}} \otimes \underbrace{W_{22}}_{\text{(2,1)}}$  


This decomposition would lead to to reduction of $28M$ ($50\%$). The new network would have approx $95M$. Our goal is to eventually reach the $82M$ mark, similar to DistilGPT2, and other Factorized models (inserts other refs here).

\subsection{Different decomposition schemes:}%
\label{sec:Different decomposition schemes}

It is reasonable to aim for a decomposition that guarantees the maximum rank we can get. Since the Rank of Kronecker products is multiplicative, i.e., $\mathrm{rank}(A\otimes B) = \mathrm{rank}(A)\cdot\mathrm{rank}(B)$ \footnote{\href{ https://math.stackexchange.com/questions/707091/elementary-proof-for-textrank-lefta-otimes-b-right-textranka-cdot}{Link to proof: https://math.stackexchange.com/questions/707091/elementary-proof-for-textrank-lefta-otimes-b-right-textranka-cdot}}, we can easily compute the rank of each possible decomposition. In our case, we have $W \in \mathbb{R}^{(m,n)}$ where $m = 3072$ and $n = 768$. Hence, for each layer of GPT2, we aim to find the ``optimal'' $A \in \mathbb{R}^{(m_1, n_1)}$, and $B\in \mathbb{R}^{(m_2, n_2)$, i.e.,: 

	\[W \approx A\otimes B, \qquad m = m_1 m_2, n = n_1 n_2 \]. 

	W.l.o.g, for each decomposition $(A,B)$ the maximum rank we can reach is $\min(m_1,n_1) \cdot \min(m_2,n_2)$. And each of the reduced decompositions would have exactly $m_1 n_1 + m_2 n_2$ parameters. Hence, theoritically, the maximum rank we can get is $768$ of a $(3072, 768)$ matrix. The following table summarizes some possible combinations, alongside the reduction it would lead to per layer, and the total number of parameters in GPT2, for only those decompostions of maximal attainable rank. We are particularly interested in 3 class of models, the 67M, the 81M and the 96M. (Need to add this) Furthermore, we add multiple factors to the models labeled with \textbf{MF} (second table / a few decompositions are missing, check this out. e.g., 3072, 384).
\begin{table}[htb!]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
Name 	& Dimension 		& 	#params 	& Model size\\
\hline
67M		&  (64, 32)         &   3200       & 67,893,504  \\ \hline 
		&  (64, 48)         &   3840       & 67,908,864  \\ \hline 
		&  (96, 32)         &   3840       & 67,908,864  \\ \hline
		&  (64, 64)         &   4672       & 67,928,832  \\ \hline
		&  (128, 32)        &   4672       & 67,928,832  \\ \hline 
		&  (96, 48)         &   5120       & 67,939,584  \\ \hline 
		&  (96, 64)         &   6528       & 67,973,376  \\ \hline
		&  (128, 48)        &   6528       & 67,973,376  \\ \hline
		&  (128, 64)        &   8480       & 68,020,224  \\ \hline 
		&  (96, 96)         &   9472       & 68,044,032  \\ \hline 
 		&  (192, 48)        &   9472       & 68,044,032  \\ \hline
		&  (128, 96)        &   12480      & 68,116,224  \\ \hline
		&  (192, 64)        &   12480      & 68,116,224  \\ \hline 
		&  (128, 128)       &   16528      & 68,213,376  \\ \hline 
MF1		&  (256, 64)        &   16528      & 68,213,376  \\ \hline
		&  \cdots &   \cdots    & \cdots \\ \hline
MF2		&  (1024, 256)      &   262153     & 74,108,376  \\ \hline 
		&  (768, 384)       &   294920     & 74,894,784  \\ \hline
		&  (1024, 384)      &   393222     & 77,254,032  \\ \hline
81M 	&  (768, 768)       &   589828     & 81,972,576  \\ \hline
		&  (1536, 384)      &   589828     & 81,972,576  \\ \hline
		&  (1024, 768)      &   786435     & 86,691,144  \\ \hline
96M		&  (1536, 768)      &   1179650    & 96,128,304  \\ \hline
GPT2	&  (3072, 768)      &   2359297    & 124,439,832 \\ \hline
\end{tabular}
\caption{Different compression schemes}
\end{table}


\begin{table}[htb!]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Name 	& Dimension 		& 	#params    & 1 factor    & 2 factors   & 3 factors     \\ \hline                                                                                 
MF1		&  (256, 64)        &   16528      & 68,2M       & 68,6  & 69M    \\ \hline
MF2		&  (1024, 256)      &   262153     & 74M         & 80M   & 86M    \\ \hline 
\end{tabular}
\caption{Adding multiple Kronecker Factors}
\end{table}


\newpage

\subsection{Initialization}%
\label{sub:Initialization}

Since we inherit a GPT2 checkpoint that was trained for multiple epochs on the Open Web Text (OWT) (cite here), we want to initiliaze our weights in a way that leverages the old pre-training as much as possible. This is of course obvious for the parameters that are common between \textbf{GPT2} and \textbf{KronyPT} (i.e., we match).  But more tricky for the weights that are decomposed into Kronecker Factors. In our work, we try two different approaches, Van Loan decomposition (cite here), and a Pruning based method exclusively for the 95M model.

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline

 & & \multicolumn{3}{c|}{Datasets} \\ \hline
\# Params &  Model            & wikitext-103 & wikitext-2 & Lambada \\ \hline
95M       & \textbf{Krony-PT1}  & 41.80        & 35.50      & 61.34         \\ \hline
95M       & \textbf{Krony-PT1}  & 41.81        & 36.02      & 59.95         \\ \hline
\end{tabular}
\caption{Comparison of different models on various datasets.}
\end{table}

\subsection{Van Loan Decomposition}

The Van Loan decomposition is a mathematical technique used to represent a matrix as a sum of Kronecker products, which is particularly useful in numerical linear algebra for simplifying various matrix computations. This method is named after Charles F. Van Loan, who developed this approach.

\subsection{Overview}

The Van Loan decomposition provides a framework to express a matrix \( Z \) in terms of Kronecker products of smaller matrices. This is beneficial in applications such as signal processing, systems theory, and solutions to tensor equations where the representation of a matrix in a reduced form can significantly simplify computations and analyses.

\subsection{Decomposition Process}

The basic principle of the Van Loan decomposition is to express a matrix \( Z \) as a sum of Kronecker products of smaller matrices. The general form of the decomposition is given by:
\begin{equation}
    Z = \sum_{i=1}^k A_i \otimes B_i,
\end{equation}
where \( A_i \) and \( B_i \) are matrices of appropriate dimensions, and \( \otimes \) denotes the Kronecker product. The challenge lies in determining the matrices \( A_i \) and \( B_i \) such that the sum of their Kronecker products faithfully reconstructs the matrix \( Z \).

\subsection{Kronecker Product}

The Kronecker product, central to this decomposition, is defined for two matrices \( A \) of size \( m \times n \) and \( B \) of size \( p \times q \) as:
\begin{equation}
    A \otimes B = 
    \begin{pmatrix}
    a_{11}B & \cdots & a_{1n}B \\
    \vdots & \ddots & \vdots \\
    a_{m1}B & \cdots & a_{mn}B
    \end{pmatrix},
\end{equation}
which results in a block matrix of size \( (mp) \times (nq) \).

\subsubsection{Van Loan Method}%
\label{sub:The fuck is Van Loan}

The Van Loan decomposition is a method based on the SVD that can be used to approximate a given matrix as an sum of \( k \) Kronecker products, optimizing the Frobenius norm of the approximation. Originated from Van Loan and Pitsianis's 1993 work on approximations with Kronecker products \cite{van1993approximation}. 

\textbf{Idea:} Given a matrix \( A \), with dimensions \((..., m \cdot m_2, n \cdot n_2)\), find  \(\{ U_i\}\) and \( \{V_i\} \), such that \( A \approx \sum_{i=1}^k U_i \otimes V_i \). The core steps of the algorithm are:

\begin{enumerate}
    \item Rearrange \( A \) to form a new matrix for decomposition.
    \item Perform a low-rank SVD to approximate \( A \) by \( U S V^T \) where \( S \) contains the top \( k \) singular values.
    \item Reshape and scale \( U \) and \( V \) matrices to match the desired Kronecker product forms.
	\item Returns \( U \) and \( V \) matrices scaled by the square root of the singular values
\end{enumerate}

The returned matrices, suitable for reconstructing \( A \) with minimized Frobenius error. The matrices \( U \) and \( V \) are of shape \( (..., k, m, n) \) for \( U \) and \( (..., k, m_2, n_2) \) for \( V \). A simple Python script to generate the U_s and V_s is as follows:


\begin{verbatim}
def kronecker_decompose(A, m: int, n: int, *, k: int = 1, niter: int = 10):

    m2, n2 = A.shape[-2] // m, A.shape[-1] // n
	
    A = rearrange(A, "... (m m2) (n n2) -> ... (m n) (m2 n2)", m=m, m2=m2, n=n, n2=n2)

    u, s, v = torch.svd_lowrank(A, q=k, niter=niter)

    u = rearrange(u, "... (m n) k -> ... k m n", m=m, n=n, k=k)
    v = rearrange(v, "... (m2 n2) k -> ... k m2 n2", m2=m2, n2=n2, k=k)

    scale = s[..., None, None].sqrt()
    return u * scale, v * scale
\end{verbatim}

\subsubsection{KronZ2: Derivation of Van Loan Decomposition}

To derive the Van Loan decomposition from SVD, consider a matrix \( Z \) that we want to express as a sum of Kronecker products. The steps are as follows:

1. **Perform SVD on \( Z \):**
   Assume \( Z \) can be decomposed via SVD as:
   \[
   Z = U_Z \Sigma_Z V_Z^T.
   \]

2. **Reshape Singular Values:**
   Convert the diagonal matrix \( \Sigma_Z \) into a matrix \( M \) appropriate for Kronecker product decomposition. This involves arranging the singular values \( \sigma_i \) along different blocks or matrices that reflect the desired Kronecker product structure.

   3. \textbf{Kronecker Product Formulation:}
   Identify matrices \( A_i \) and \( B_i \) such that their Kronecker product corresponds to the components in \( U_Z \), \( M \), and \( V_Z^T \). Typically, \( A_i \) can be chosen as different columns or blocks of \( U_Z \), and similarly, \( B_i \) can be derived from \( V_Z \).

   The key is to express each singular vector product term from SVD as a sum of Kronecker products:
   \[
   Z = \sum_{i=1}^r \sigma_i (u_i \otimes v_i),
   \]
   where \( u_i \) and \( v_i \) are vectors or blocks from \( U_Z \) and \( V_Z \), respectively, and \( r \) is the rank of \( Z \).





\subsubsection{Pruning based Initialization}%
\label{sub:Pruning based Initialization}


We propose a new initialization strategy by inducing sparsity in the first factor of the Kronecker Product, and prune it by half. This is equivalent to picking the second factor as 
\begin{bmatrix}
1 \\
0
\end{bmatrix}. Now, we illustrate how this procedure works with a random matrix.

\[
\begin{bmatrix}
a_{11} & a_{12} & a_{13} & a_{14} & a_{15} \\
a_{21} & a_{22} & a_{23} & a_{24} & a_{25} \\
a_{31} & a_{32} & a_{33} & a_{34} & a_{35} \\
a_{41} & a_{42} & a_{43} & a_{44} & a_{45} \\
a_{51} & a_{52} & a_{53} & a_{54} & a_{55} \\
a_{61} & a_{62} & a_{63} & a_{64} & a_{65} \\
\end{bmatrix}
\xrightarrow[\text{pruning}]{}
\begin{bmatrix}
a_{11} & a_{12} & a_{13} & a_{14} & a_{15} \\
0 & 0 & 0 & 0 & 0  \\
a_{31} & a_{32} & a_{33} & a_{34} & a_{35} \\
0 & 0 & 0 & 0 & 0  \\
a_{51} & a_{52} & a_{53} & a_{54} & a_{55} \\
0 & 0 & 0 & 0 & 0  \\
a_{61} & a_{62} & a_{63} & a_{64} & a_{65} \\
\end{bmatrix}
=
\begin{bmatrix}
a_{11} & a_{12} & a_{13} & a_{14} & a_{15} \\
a_{31} & a_{32} & a_{33} & a_{34} & a_{35} \\
a_{51} & a_{52} & a_{53} & a_{54} & a_{55} \\
a_{61} & a_{62} & a_{63} & a_{64} & a_{65} \\
\end{bmatrix}
\otimes
\begin{bmatrix}
1  \\
0  
% Elements of matrix D
\end{bmatrix}
 

\]

\section{Experiements}%
\label{sub:Experiements}

\subsection{Training setup}%
\label{sub:Setup}

For pre-training, we follow general industry standards, namely of Chinchilla \cite{hoffmann2022training}. We noticed more stable loss curves with higher \textbf{batch sizes}, typically $128$ batch size (attained with a gradient accumulation of $4$, and each forward pass seeing a batch of $32$), and constant \textbf{learning rate} of $6 \times 10^{-5}$ (better than a cosine scheduler with a warm-up). After pre-training on approximately one epoch, we pick the best checkpoint and train further on $\approx$ 500k tokens per step (as suggested in the literature) which amounts in our set up (sequence length of $1024$) to a batch size of $512$. All experiments have been conducted using a single A100 80 GB, for one epoch of on Open Web Text \cite{Gokaslan2019OpenWeb}, an open source reproduction of OpenAI's WebText dataset.


After training for approximately one epoch, we usually ``fine-tune" aggressively with very large batch size (approx 500k tokens per iteration) for approx. $10\%$ of data. We had best success with this strategy.


\subsection{Results}%
\label{sub:results}

We refer to our models with the nomenclature \textbf{Krony-PT-XM\{-Y\}}, with $X$ representing the number of the parameters, and optionaly $Y$ referring to separate checkpoints of the same model class. We namely focus on 2 class models, the  \textbf{Krony-PT-81M} and \textbf{Krony-PT-95M}. We evaluate our models next-token prediction capabilities on 3 datasets, wikitext103, wikitext2 \cite{merity2016pointer} and Lambada \cite{paperno2016lambada}. 

\subsection{The 81M class:}%
\label{sub:The 81M class:}

\textbf{Krony-PT-81M} is our model class of 81M parameters, and the suffixes $1350$ and $3950$ refer to different checkpoints. Both models clearly outperform distilGPT2 (\cite{sanh2019distilbert}) on the 3 datasets, with a noticeable better perplexity on Lambada \cite{paperno2016lambada}.


\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
 & & \multicolumn{3}{c|}{Datasets} \\ \hline
\# Params &  Model            & wikitext-103 & wikitext-2 & Lambada \\ \hline
124M      & GPT2              & 29.16        & 24.67      & 45.28      \\ \hline
82M       & DistilGPT2        & 44.53        & 36.48      & 76.00      \\ \hline
81M       & \textbf{KronyPT-81M-1350}  & \textbf{41.98}   & \textbf{34.99}      & -          \\ \hline
81M       & \textbf{KronyPT-81M-3950}  & -            	  & -          & \textbf{64.92}      \\ \hline
\end{tabular}
\caption{Perplexity results of Krony-PT and DistilGPT}
\end{table}


\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
 & & \multicolumn{3}{c|}{Datasets} \\ \hline
\# Params &  Model            & wikitext-103 & wikitext-2 & Lambada \\ \hline
81M       & \textbf{KronyPT-81M-1350}  & 41.98        & 34.99      & -          \\ \hline
81M       & \textbf{KronyPT-81M-3950}  & -            & -          & 64.92      \\ \hline
119M       & TQCompressedGPT2  & 40.28        & 32.25      & 64.72      \\ \hline
119M       & KEPT-2   & 40.97        & 32.81      & 67.62      \\ \hline
\end{tabular}
\caption{Perplexity of Krony-PT against other Kronecker based models.}
\end{table}

\subsection{The 95M class:}%
\label{sub:The 95M class:}

\subsection{Scale test}%
\label{sub:Scale test}
How far can we push down the size? 

Convergence starts to emerge starting from 80M models.   80M with single factors is higher than 80M with small dims and high factors.

\subsection{Multiple Kronecker factors, with scalers.}%
\label{sub:Multiple Kronecker factors, with scalers.}

In this section, we study the effect of adding multiple Kronecker Factors, and additionally adding scalers. In this setting, we compare 3 models:

\textbf{Setting number 1} $\rightarrow$ 96M class (i.e., the biggest model class with one factor)

\textbf{Setting number 2} $\rightarrow W = \sum_{n=1}^{r} A_i \otimes B_i $ 

\textbf{Setting number 3} $\rightarrow W  = \sum_{n=1}^{r} s_i (A_i \otimes B_i)$

To keep things at a comparable level, we train this class of 92.2M parameters, attained with $4$ factors of size $(256, 1024)$. 


\textbf{Note:} Adding scalers barely adds any complexity to the parameter count, for the setting number 3, we add exactly $96$ parameters, depicted as follows: $96 = (\underbrace{4}_{\text{number of factors}} \times \underbrace{2}_{\text{MLP matrices per layer}}) \times \underbrace{12}_{\text{number of layers}}$. Moreover, it doesn't add any additional latency to inference compared to only using multiple factors (without scalers), as we can multiply the scaler $s_i$ with the first Kronecker factor $A_i$ first and then compute the Kronecker product with $B_i$.

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
 & & \multicolumn{3}{c|}{Datasets} \\ \hline
\# Params &  Model            & wikitext-103 & wikitext-2 & Lambada \\ \hline
96M       & \textbf{KronyPT 96M Parameters}  & 41.80        & 35.50      & 61.34         \\ \hline
92.2M     & \textbf{4 factors without scalers} & -            & -          &       \\ \hline
92.2M     & \textbf{4 factors with scalers}  &  \textbf{38.15}   & \textbf{33.08}      & \textbf{61.04} \\ \hline
\end{tabular}
\caption{Effect of adding scalers to multiple Kroneckers}

\end{table}

\textbf{Conclusion:} Adding multiple factors do not help much, but adding scalers can during training improve model performance.

\subsection{Embeddings freezing:}%
\label{sub:Embeddings freezing:}

In this setting we test the impact of freezing on the model's performance. I should probb test on the 82M models. Code should be done today. We also noticed that the scalers in the case of 4 factors, -- no matter what the initialization is --, always do converge to around 4. Hence, we tried to to freeze them as well at 4. And see how the learning would react.


\subsection{Aggressive learning rate.}%

As opposed to Chinchilla recommendations, we noticed that having an aggressive learning rate helps convergence. (to be confirmed)

\subsection{General trends:}%

Things to add here:

\begin{itemize}
	\item Models do not get better at Wiki, but do improve on Lambada.
	\item Distillation is not helpful 
	\item Does pruning help? For training and for inference?
	\item does re-plugging the matrix work? 
\end{itemize}

\section{Discussion}%
\label{sec:Discussion}

We clarify in this section some design choices of the other papers:

\subsection{Parameter count:}%
\label{sub:Parameter count:}

The other papers (\cite{tahaei2022kroneckerbert}, \cite{abronin2024tqcompressor}), count the number of parameters differently than how our method and DistilGPT2 do, which makes the comparison not tit for tat. The difference is that they do not include the output matrix in the parameter count, which is not of a negligible size (approximately 40M), especially when initialized using pre-trained GPT2 weights. Quoting from KnGPT2 \cite{tahaei2022kroneckerbert}: ``Note that number of parameters of the models are reported excluding the output embedding layer in language modelling which is not compressed, is equal to row Parameters''. TQCompressor \cite{abronin2024tqcompressor} follow their lead as well, which was clarified on a GitHub issue \footnote{\href{https://github.com/terra-quantum-public/TQCompressedGPT2/issues/1}{Link to GitHub issue: https://github.com/terra-quantum-public/TQCompressedGPT2/issues/1}}.
This difference in counting, makes their 81M models, a 120M in reality. The reason why we (and GPT2 paper) don't count the output matrix is because it is identical to the embedding matrix, this known as weight tying or weight sharing. Which does not seem the case with the other papers (I have contacted the other paper for clarifications through github).

\subsection{Only 3\% of data}%
\label{sub:Only 3\% of data}

Both papers claim that they only used 3\% of the training data to train their models. We argue that in this set up, limiting your training data to only a fraction  does not imply a better compression/factorization method, for (the simple and) following reasons:

\begin{enumerate}
	\item They inherit almost half the weights from the old GPT2 checkpoint that was trained for numerous epochs, and has seen the whole data. 
	\item They use Van Loan (VL) method to initialize the Kronecker products, hence, even, when they don't match the original checkpoint, VL is not a random initialization, some knowledge is definitely passed through the SVD. 
	\item The use the same output matrix as GPT2, without weight trying (one can be fairly sure that this matrix ``distills" all knowledge that was learned). 
\end{enumerate}

Hence, we can't quantify exactly ``how much knowledge" has been passed through the already trained parameters. A fair comparison would be to initialize \textbf{all the parameters} of the new compressed model with random values, and not rely on any of the other pre-trained ones. Which is clearly not the case here.


\newpage
\bibliography{bib}
\bibliographystyle{ieeetr}


\end{document}
