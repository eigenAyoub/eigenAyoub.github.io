<p>This mini blog answers:</p>

<ol>
  <li>What is the famous  the famous perplexity score?</li>
  <li>How is it implemented, especially in Hugging Face?</li>
  <li>Beyond perplexity?</li>
</ol>

<h2 id="quick-wiki">Quick wiki:</h2>

<p>Perplexity is defined as the exponentiated average negative log-likelihood of a sequence. If we have a tokenized sequence \(X = (x_0, x_1, \ldots, x_t)\), then the perplexity of \(X\) is,</p>

\[\text{ppl}(X) = \exp 
\left\{ 
-\frac{1}{t} \sum_{i=1}^{t} \log p_{\theta}(x_i \mid x_{&lt; i}) 
\right\}\]

<p>Obviously here \(p_{\theta}(x_i \mid x_{&lt; i})\) refers to softmaxe’d output that the autoregressive model assigns to the token \(x_i\) after the seeing the past sequence \(x_{&lt;i}\).</p>

<h2 id="code">Code:</h2>

<p>The following code snippet computes the perplexity score of GPT2-small on Lambada:</p>

<details>
	<summary> Click show pre-processing (loading the model, dataset, and configs)</summary>
<div>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2LMHeadModel</span><span class="p">,</span> <span class="n">GPT2TokenizerFast</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>  

<span class="n">device</span> <span class="o">=</span> <span class="s">"cuda"</span>

<span class="n">model</span>  <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="sa">f</span><span class="s">"./hf/73150"</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2TokenizerFast</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s">"gpt2"</span><span class="p">)</span>

<span class="n">test</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s">"lambada"</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s">"test"</span><span class="p">)</span>
<span class="n">encodings</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s">"</span><span class="se">\n\n</span><span class="s">"</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">test</span><span class="p">[</span><span class="s">"text"</span><span class="p">]),</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s">"pt"</span><span class="p">)</span>

<span class="n">max_length</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">n_positions</span>
<span class="n">stride</span> <span class="o">=</span> <span class="mi">1024</span>
<span class="n">seq_len</span> <span class="o">=</span> <span class="n">encodings</span><span class="p">.</span><span class="n">input_ids</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="n">nlls</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">prev_end_loc</span> <span class="o">=</span> <span class="mi">0</span>
</code></pre></div>    </div>
  </div>

</details>

<p>The main loop:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">begin_loc</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">stride</span><span class="p">)):</span>
    <span class="n">end_loc</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">begin_loc</span> <span class="o">+</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>
    <span class="n">trg_len</span> <span class="o">=</span> <span class="n">end_loc</span> <span class="o">-</span> <span class="n">prev_end_loc</span>  
	<span class="c1"># may be different from stride on last loop
</span>    <span class="n">input_ids</span> <span class="o">=</span> <span class="n">encodings</span><span class="p">.</span><span class="n">input_ids</span><span class="p">[:,</span> <span class="n">begin_loc</span><span class="p">:</span><span class="n">end_loc</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">target_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">.</span><span class="n">clone</span><span class="p">()</span>
    <span class="n">target_ids</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="n">trg_len</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">100</span>

    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">target_ids</span><span class="p">)</span>
        <span class="n">neg_log_likelihood</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">loss</span>

    <span class="n">nlls</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">neg_log_likelihood</span><span class="p">)</span>

    <span class="n">prev_end_loc</span> <span class="o">=</span> <span class="n">end_loc</span>
    <span class="k">if</span> <span class="n">end_loc</span> <span class="o">==</span> <span class="n">seq_len</span><span class="p">:</span>
        <span class="k">break</span>

<span class="n">ppl</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">nlls</span><span class="p">).</span><span class="n">mean</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="n">ppl</span><span class="p">.</span><span class="n">item</span><span class="p">())</span>
</code></pre></div></div>

<p>Let’s dissect the main loop. Matching the equation above and <code class="language-plaintext highlighter-rouge">ppl = torch.exp(torch.stack(nlls).mean())</code>, <code class="language-plaintext highlighter-rouge">nlls[i]</code> must represent the \(\log p_{\theta}(x_i \mid x_{&lt; i})\).</p>

<p>Question is, what is the <code class="language-plaintext highlighter-rouge">outputs = model(input_ids, labels=target_ids)</code>.</p>

<p>The variable <code class="language-plaintext highlighter-rouge">outputs</code> is of type <code class="language-plaintext highlighter-rouge">transformers.modeling_outputs.CausalLMOutputWithCrossAttentions</code>, and has three keys:</p>

<ol>
  <li><code class="language-plaintext highlighter-rouge">outputs.loss</code>: a single scaler, it represents exactly the quantity \(\log p_{\theta}(x_i \mid x_{&lt; i})\).</li>
  <li><code class="language-plaintext highlighter-rouge">outputs.logits</code>: this is the actual output matrix of the LM, is has a shape of <code class="language-plaintext highlighter-rouge">[1, seq_len, vocab_len]</code>.</li>
  <li><code class="language-plaintext highlighter-rouge">past_key_values</code> will ignore for now.</li>
</ol>

<p>How to compute the <code class="language-plaintext highlighter-rouge">loss</code> from the <code class="language-plaintext highlighter-rouge">logits</code>:</p>

<p>We have:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">target_ids</span><span class="p">)</span>
<span class="c1"># input_ids &gt;&gt; tensor([[  257,  1598,  7815,  ...,  1175, 32002,   379]], device='cuda:0')
# target_ids &gt;&gt; a clone of input_ids. 
</span></code></pre></div></div>

<p>Hence, for each run:</p>

<ul>
  <li>We are only interested in the model prediction for the <strong>BEFORE</strong> last token, which is <strong>32002</strong> in this example.</li>
  <li>We need to look at <code class="language-plaintext highlighter-rouge">outputs[0,-2]</code> and not <code class="language-plaintext highlighter-rouge">outputs[0,-1]</code>.</li>
  <li><code class="language-plaintext highlighter-rouge">outputs[0, -2]</code> has a <code class="language-plaintext highlighter-rouge">[1, vocab_size]</code> shape. And, <code class="language-plaintext highlighter-rouge">outputs[0, -2][379]</code> would be represent exactly how much weight does the model think that the next token after 32002 would be <strong>379</strong>.</li>
  <li><code class="language-plaintext highlighter-rouge">outputs[0, -2]</code> is not normalized. Hence, it should be softmaxe’d first.</li>
</ul>

<h2 id="references">References:</h2>

<ul>
  <li><a href="https://huggingface.co/docs/transformers/en/perplexity">Hugging Face blog</a></li>
  <li><a href="https://thegradient.pub/understanding-evaluation-metrics-for-language-models/">The gradient blog</a></li>
</ul>

