---
layout: post
title:
permalink: /blogs/TD-learning/
---

Keywords: RL, Q-learning, stochastic approximation.


$$p(s',r \lvert s,a)=\Pr \{S_t=s',R_t=r \lvert S_{t-1}=s,A_{t-1}=a\}$$

## Proof of convergence of Q-Learning


In this blogpost, I'll write about what I have learned regarding the asymptomatic guarentees of Q-Learning and TD-Learning. Eventhough we are going to use some results from Stochastic Approximation (SA) theory without a proof, going through this material made me appriciate Q-learning way more than just a toy tool.

It is also fair to meantion that the "asymtotic" convergence of Q-Learning is fairly \*EASY\* compared to finding guanrentees on the \* RATE *\ of convergence, which is still an active field of research, I guess! Props to you RL theory folks!

At first I'll formulate the problem setting in a *SA framework*, state the conditions and assumptions, and  finally I'll the main theorem to the case of Q-learning.


## Content:
* [Setting](#setting)
* [Algorithm](#algorithm)
* [Proof](#proof)
	* [Sketch](#sketch)
	* [Proof](#proof)
* [References](#references)


## Stochastic Approximation of a contractive operator:

Let $$L: R^d \longrightarrow R^d$$ be a $$c$$-lipschitz function where $$c<1$$ (This obviously implies contraction, which implies the fixed point theorem in Banach Spaces). We are interested in finding  $$\theta^*$$ the fixed point of $$L$$, i.e., $$L \theta^* = \theta^*$$.


> Bear in mind that the Q-learning actor that would assume the role of $$L$$ is the Bellman Optimality Operator $$T^*$$. As you know the $$T^*$$ is a contraction mapping for the infinite? --- check this out today

> While $$\theta_{t}$$ would be replaced by $$Q_t(.,.)$$, therefore, the dimension $$d$$ of $$
\theta_{t}$$ should match the state-action space cardinal $$\lvert X x A \lvert$$.

With the right conditions on $$\{ \alpha_{t} \}_{t=>0}$$ (Known as the Monro ... SA conditions 1 below, kn), having access to $$L$$ guarantees us to land in $$\theta^*$$ using the following (smooth) interative update:

$$ \theta_{t+1} = (1-\alpha_t) \theta_{t} + \alpha_t L \theta_{t}$$ 
 
SA conditions:
 1. Divergence of  $$\{ \alpha_{t} \}_{t=>0}$$
 2. Convergence of  $$\{ \alpha_{t}^2 \}_{t=>0}$$

 Intuitively speaking, we the want ... to slowly diverge. 

The first condition ensures the mean, the second is crutial to make the variance of the estimate vanish to 0.

The past result is a straight application of the fixed point theorem in Banach Spaces, for $$ L' = (1- \alpha)I + \alpha L$$.

Having access to $$L$$ is a luxary we can't generally afford in online settings, where the model dynamics are agnostic. A more realistic situation is to only have access to a noisy \*unbiased*\ estimate, namely $$ L \theta_{t} +  \eta_{t} $$ with $$ \mathbb{E}[\eta_{t}] = 0 $$.


To make it even more interesting, we want to update each componenent apart, $$\theta_{t}^i$$ for $$i \partof [1:d]$$ as follows:

$$\alpha_{j}$$ = 0 for $$ j |= i $$   [#]

> With our analogy in mind, in Q-Learning, we don't update Q_t synchronously for all state-action pair. 

Therefore, we want to prove that the sequence of $$\theta_{t}$$ generated by  procedure above would eventully converge to the fixed point? the answer is YES [x-ref], provided some assumptions of course. Nothing comes for free in this business! Namely o the noise $$ \etha $$

Let's call them Assuption A1 following [1] notations:

Assumption A1:
1- Regarding the mean:
This assuption makes the estimate unbiased

2 Regarding the variance:

Now let's state the theorem: Convergence of stochastce approximation:



Usually, the conditions in A2, especially the iones regarding the variance are the "hardest" to prove. The other ones are usually obvious. The operator would usually be T_star which a contraction w.r.t the inf_norm and the {alpha_t} sastifies the SA conditions by design. 

Let's see in Q-Learning for instance. The update rule looks like:

----

When re-formulated:

----

N_t = ....

we get ...


**Remark How mayn times do we need to visit a pair $$\( X,A \)$$**

A condition that we never stated regarding how we explore the state-action space. As the \alpha_{t} needs to be verified for each parin $$(X,A)$$, this requires us to visit each state infinelty many... otherwise the first condition wouldn't be verified.


Of course this is ony an asymptotic guaarntee. 

## Setting:
We have an MDPs $$ $$
Take it from Preux Papers, Check 2 or 3 papers


## Algorithm:

The interaction between the agent and its environment is captured through an MDP, a 5-tuple <X,A, P, R, gamma>. WE aim to learn an Optimal Policy by first learning the Optimal Action-Value function, Q^* for each pair x,a in X x A.

The asynchronous Q-Learning is as follows:

Note that this is an off-policy RL algorithm, as the behavioral p[olicy differes fro the one we are trying evaluaatin (\pi_{g}(Q_t)

## Proof:
$$
\begin{aligned}
V(s) &= \mathbb{E}[G_t \vert S_t = s] \\
&= \mathbb{E} [R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots \vert S_t = s] \\
&= \mathbb{E} [R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \dots) \vert S_t = s] \\
&= \mathbb{E} [R_{t+1} + \gamma G_{t+1} \vert S_t = s] \\
&= \mathbb{E} [R_{t+1} + \gamma V(S_{t+1}) \vert S_t = s]
\end{aligned}
$$


### Sketch:



### Proof:
Let this and that be ...


## References:
* [] Neuro dynamic programming
* [] Link to TD-learnig paper
* [] Amir massoud Fahramad
* [] Link to youtube playlist, github linl

